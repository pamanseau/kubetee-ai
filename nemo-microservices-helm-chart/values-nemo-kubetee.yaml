tags:
  # -- When set to `true`, installs all NeMo microservices. To install individual microservices, set `platform` to `false` and configure the `tags` key with values for each individual microservice. For more information, refer to [Tag-Based Installation](https://docs.nvidia.com/nemo/microservices/latest/set-up/deployment-options.html#tag-based-installation).
  platform: true
  # -- Specifies whether to install the NeMo Auditor microservice.
  auditor: false

# The chart generates top-level secrets (custom-values.yaml overrides these)

# -- You can use an existing Kubernetes secret for pulling images. The chart uses the `ngcAPIKey` value to generate the secret if you set this to an empty string.
existingSecret: ngc-api

# -- You can specify an existing Kubernetes image pull secret for pulling images from the container registry. The chart uses the `ngcAPIKey` value to generate the secret if you set this to an empty string.
existingImagePullSecret: nvcrimagepullsecret

# -- Your NVIDIA GPU Cloud (NGC) API key authenticates and enables pulling images from the NGC container registry. The existing secret overrides this key if you provide one to the `existingSecret` key.
ngcAPIKey: YOUR-NGC-API-KEY

# -- List of image pull secrets. Existing secrets override these values if you specify them. Use this only for experimentation when you want to hardcode a secret in your values file.
# @default -- `[{"name":"nvcrimagepullsecret","password":"YOUR-NGC-API-KEY","registry":"nvcr.io","username":"$$oauthtoken"}]`
imagePullSecrets:
  - name: nvcrimagepullsecret
    registry: nvcr.io
    username: $oauthtoken
    password: YOUR-NGC-API-KEY

global:
  # -- The name of the image pull secret to use globally across all services.
  imagePullSecrets:
    - name: nvcrimagepullsecret

data-store:
  # -- Global parameters to override the same settings in all subcharts of the data-store Helm chart.
  global:
    # -- Global image registry.
    imageRegistry: ""
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    # -- Global image pull secrets.
    imagePullSecrets: []
    # -- Global storage class that applies to persistent volumes.
    storageClass: ""
    # -- Global host aliases which will be added to the pod's hosts files.
    hostAliases: []
    # - ip: 192.168.137.2
    #   hostnames:
    #   - example.com

  # -- Number of replicas for the deployment.
  replicaCount: 1

  # -- Strategy configuration for controlling how pod updates are performed.
  # @default -- This object has the following default values for the strategy configuration.
  strategy:
    # -- The strategy type for pod updates. Use `RollingUpdate` if you use `ReadWriteMany` persistent storage or deploying on a single node. Otherwise, use `Recreate` to allow upgrades, although it causes downtime during the upgrade process.
    type: "Recreate"
    rollingUpdate:
      # -- Maximum number of pods that can be created above the desired amount during an update.
      maxSurge: "100%"
      # -- Maximum number of pods that can be unavailable during an update.
      maxUnavailable: 0

  # -- Kubernetes cluster domain name.
  clusterDomain: cluster.local

  # -- Container image configuration settings
  # @default -- This object has the following default values for the image configuration.
  image:
    # -- The registry where the NeMo Data Store image is located.
    registry: "nvcr.io"
    # -- The repository path of the NeMo Data Store image.
    repository: "nvidia/nemo-microservices/datastore"
    # -- The image tag to use.
    tag: ""
    # -- The image digest to use for more precise version control.
    digest: ""
    # -- The image pull policy determining when to pull new images.
    pullPolicy: IfNotPresent
    # -- Whether to run the container with rootless security context.
    rootless: true
    # -- Complete override string for the image specification.
    fullOverride: ""

  # -- Configuration for image pull secrets to access private registries.
  imagePullSecrets:
    # -- Name of the secret containing registry credentials.
    - name: nvcrimagepullsecret

  # -- Pod-level security context settings
  podSecurityContext:
    # -- The file system group ID to use for all containers.
    fsGroup: 1000
    # -- Set the permission change policy for mounted PVCs.
    fsGroupChangePolicy: OnRootMismatch

  # -- Container-level security context settings
  containerSecurityContext: {}
  #   allowPrivilegeEscalation: false
  #   capabilities:
  #     drop:
  #       - ALL
  #   # Add the SYS_CHROOT capability for root and rootless images if you intend to
  #   # run pods on nodes that use the container runtime cri-o. Otherwise, you will
  #   # get an error message from the SSH server that it is not possible to read from
  #   # the repository.
  #     add:
  #       - SYS_CHROOT
  #   privileged: false
  #   readOnlyRootFilesystem: true
  #   runAsGroup: 1000
  #   runAsNonRoot: true
  #   runAsUser: 1000

  # -- **DEPRECATED** Run init and NeMo Data Store containers as a specific securityContext. The securityContext variable has been split two: `containerSecurityContext` and `podSecurityContext`.
  securityContext: {}

  # -- Pod disruption budget configuration.
  podDisruptionBudget: {}
  #  maxUnavailable: 1
  #  minAvailable: 1

  # -- Service configuration for exposing the application.
  # @default -- This object has the following default values for the service configuration.
  service:
    # -- HTTP service configuration.
    http:
      # -- The Kubernetes service type to create for HTTP traffic.
      type: ClusterIP
      # -- The port number to expose for HTTP traffic.
      port: 3000
      # -- The cluster IP address to assign to the service.
      clusterIP: ""
      # -- The static IP address for LoadBalancer service type.
      loadBalancerIP:
      # -- The node port number when using NodePort service type
      nodePort:
      # -- External traffic policy for controlling source IP preservation
      externalTrafficPolicy:
      # -- List of external IP addresses to assign to the service
      externalIPs:
      # -- IP family policy for dual-stack support
      ipFamilyPolicy:
      # -- List of IP families to use for the service
      ipFamilies:
      # -- List of CIDR ranges allowed to access the LoadBalancer
      loadBalancerSourceRanges: []
      # -- Additional annotations for the HTTP service
      annotations: {}
      # -- Additional labels for the HTTP service
      labels: {}
    # -- SSH service configuration.
    # @default -- This object has the following default values for the SSH service configuration.
    ssh:
      # -- Whether to enable SSH service.
      enabled: false
      # -- The Kubernetes service type to create for SSH traffic.
      type: ClusterIP
      # -- The port number to expose for SSH traffic.
      port: 22
      # -- The cluster IP address to assign to the service.
      clusterIP: None
      # -- The static IP address for LoadBalancer service type.
      loadBalancerIP:
      # -- The node port number when using NodePort service type.
      nodePort:
      # -- The external traffic policy for controlling source IP preservation.
      externalTrafficPolicy:
      # -- List of external IP addresses to assign to the service.
      externalIPs:
      # -- IP family policy for dual-stack support.
      ipFamilyPolicy:
      # -- List of IP families to use for the service.
      ipFamilies:
      # -- The host port number when using HostPort service type.
      hostPort:
      # -- List of CIDR ranges allowed to access the LoadBalancer.
      loadBalancerSourceRanges: []
      # -- Additional annotations for the SSH service.
      annotations: {}
      # -- Additional labels for the SSH service.
      labels: {}

  # -- Ingress configuration.
  # @default -- This object has the following default values for the ingress configuration.
  ingress:
    # -- Whether to enable ingress.
    enabled: false
    # -- The Ingress class name.
    className:
    # -- Additional annotations for the Ingress. For example, `kubernetes.io/ingress.class: nginx`, `kubernetes.io/tls-acme: "true"`.
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    # -- (list) A list of maps, each containing the keys `host` and `paths` for the ingress resource. You must specify a list for configuring ingress for the microservice.
    hosts: {}
    #   - host: datastore.example.com
    #     paths:
    #       - path: /
    #         pathType: Prefix
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - datastore.example.com
    # Mostly for argocd or any other CI that uses `helm template | kubectl apply` or similar
    # If helm doesn't correctly detect your ingress API version you can set it here.
    # apiVersion: networking.k8s.io/v1

  # -- Kubernetes deployment resources configuration.
  # It is recommended to not specify default resources and to leave this as a conscious
  # choice. This also increases chances that the chart will run on environments with little
  # resources, such as minikube. If you want to specify resources, use the following
  # example, adjust the values as necessary, and remove the empty curly braces `{}`.
  # `limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi`
  resources:
    {}

  # -- The name of the alternate scheduler to use. For more information, see [Configure Multiple Schedulers](https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/).
  schedulerName: ""

  # -- NodeSelector configuration for the deployment.
  nodeSelector: {}

  # -- Tolerations configuration for the deployment.
  tolerations: []

  # -- Affinity configuration for the deployment.
  affinity: {}

  # -- TopologySpreadConstraints configuration for the deployment.
  topologySpreadConstraints: []

  # -- dnsConfig configuration for the deployment.
  dnsConfig: {}

  # -- priorityClassName configuration for the deployment.
  priorityClassName: ""

  # -- Deployment configuration.
  deployment:
    # -- How long to wait until forcefully kill the pod.
    terminationGracePeriodSeconds: 60
    # -- Labels for the deployment.
    labels: {}
    # -- Annotations for the Datastore deployment to be created.
    annotations: {}

  # -- Additional environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}.
  env:
    {}

  # -- Service account configuration.
  serviceAccount:
    # -- Whether to create a service account.
    create: true
    # -- Name of the created service account, defaults to release name. Can also link to an externally provided service account that should be used.
    name: "gitea"
    # -- Enable/disable auto mounting of the service account token.
    automountServiceAccountToken: false
    # -- Image pull secrets, available to the service account. To add a list of image pull secrets, remove `[]`use the following format: - name: private-registry-access.
    imagePullSecrets: []
    # -- Custom annotations for the service account.
    annotations: {}
    # -- Custom labels for the service account.
    labels: {}

  # -- Persistence volume configuration.
  # @default -- This object has the following default values for the persistence volume configuration.
  persistence:
    # -- Whether to enable persistent volume.
    enabled: true
    # -- Whether to create the persistent volume claim for shared storage.
    create: true
    # -- Whether to mount the persistent volume claim.
    mount: true
    # -- Name of the persistent volume claim. You can use an existing claim to store repository information.
    claimName: datastore-shared-storage
    # -- The size of the persistent volume.
    size: 100Gi
    # -- Access modes for the persistent volume.
    accessModes:
      - ReadWriteOnce
    # -- Labels for the persistence volume claim.
    labels: {}
    # -- Name of the storage class to use.
    storageClass:
    # -- Subdirectory of the volume to mount at.
    subPath:
    # -- Name of persistent volume in PVC.
    volumeName: ""
    # -- Annotations for the persistence volume claim.
    annotations:
      helm.sh/resource-policy: keep

  # -- Additional volumes to mount to the Datastore deployment.
  extraVolumes: []
  # - name: postgres-ssl-vol
  #   secret:
  #     secretName: Datastore-postgres-ssl

  # -- Mounts that are only mapped into the Datastore runtime/main container, to e.g. override custom templates.
  extraContainerVolumeMounts: []

  # -- Mounts that are only mapped into the init-containers. Can be used for additional preconfiguration.
  extraInitVolumeMounts: []

  # -- **DEPRECATED** Additional volume mounts for init containers and the Datastore main container. This value is split into the following two variables: `extraContainerVolumeMounts` and `extraInitVolumeMounts`.
  extraVolumeMounts: []
  # - name: postgres-ssl-vol
  #   readOnly: true
  #   mountPath: "/pg-ssl"

  # -- Init container Bash shell scripts. For example, to mount a client certificate when connecting to an external Postgres server, you might add commands similar to the following:
  #   `mkdir -p /data/git/.postgresql`,
  #   `cp /pg-ssl/* /data/git/.postgresql/`,
  #   `chown -R git:git /data/git/.postgresql/`,
  #   `chmod 400 /data/git/.postgresql/postgresql.key`
  initPreScript: ""

  # -- Kubernetes resource limits for init containers.
  initContainers:
    resources:
      # -- Kubernetes resource limits for init containers.
      limits: {}
      # -- Kubernetes cpu resource limits for init containers.
      requests:
        cpu: 100m
        memory: 128Mi

  # -- Signing configuration.
  signing:
    # -- Enable commit/action signing.
    enabled: false
    # -- GPG home directory.
    gpgHome: /data/git/.gnupg
    # -- Inline private GPG key for signed NeMo Data Store actions.
    privateKey: ""
    # privateKey: |-
    #   -----BEGIN PGP PRIVATE KEY BLOCK-----
    #   ...
    #   -----END PGP PRIVATE KEY BLOCK-----
    # -- Use an existing secret to store the value of `signing.privateKey`.
    existingSecret: ""

  # -- Admin user configuration settings.
  admin:
    # -- Username for the NeMo Data Store admin user.
    username: datastore_admin
    # -- Use an existing secret to store admin user credentials. For example, `datastore-admin-secret`.
    existingSecret:
    # -- Password for the NeMo Data Store admin user.
    password: s3aJPHD9!bt6d0I
    # -- Email for the NeMo Data Store admin user.
    email: "datastore@local.domain"

  metrics:
    # -- Enable NeMo Data Store metrics. Also requires setting env variable GITEA__metrics__ENABLED: "true"
    enabled: false
    serviceMonitor:
      # -- Enable NeMo Data Store metrics service monitor.
      enabled: false

  # -- LDAP configuration.
  ldap:
    []
    # - name: "LDAP 1"
    #  existingSecret:
    #  securityProtocol:
    #  host:
    #  port:
    #  userSearchBase:
    #  userFilter:
    #  adminFilter:
    #  emailAttribute:
    #  bindDn:
    #  bindPassword:
    #  usernameAttribute:
    #  publicSSHKeyAttribute:

  # Either specify inline `key` and `secret` or refer to them via `existingSecret`
  # -- OAuth configuration.
  oauth:
    []
    # - name: 'OAuth 1'
    #   provider:
    #   key:
    #   secret:
    #   existingSecret:
    #   autoDiscoverUrl:
    #   useCustomUrls:
    #   customAuthUrl:
    #   customTokenUrl:
    #   customProfileUrl:
    #   customEmailUrl:

  # -- NeMo Data Store configuration.
  # @default -- This object has the following default values for the NeMo Data Store configuration.
  config:
    # -- Application name.
    APP_NAME: "Datastore"
    # -- Runtime mode (prod/dev).
    RUN_MODE: prod
    server:
      # -- HTTP port for web interface.
      HTTP_PORT: 3000
      # -- Enable SSH server.
      START_SSH_SERVER: false
      # -- Enable LFS server.
      LFS_START_SERVER: true
    cron.GIT_GC_REPOS:
      # -- Enable git garbage collection.
      enabled: false
    lfs:
      # -- Storage type for LFS (local/s3).
      STORAGE_TYPE: local
    session:
      # -- Session provider type.
      PROVIDER: memory
    cache:
      # -- Cache adapter type.
      ADAPTER: memory
    queue:
      # -- Queue type.
      TYPE: dummy
    database:
      # -- Database type.
      DB_TYPE: postgres

  # -- Additional configuration from secret or configmap.
  additionalConfigSources: []
  #   - secret:
  #       secretName: Datastore-app-ini-oauth
  #   - configMap:
  #       name: Datastore-app-ini-plaintext

  # -- Additional configuration sources from environment variables.
  additionalConfigFromEnvs: []

  # -- Annotations for the Datastore pod.
  podAnnotations: {}

  # -- Configure OpenSSH's log level. Only available for root-based Datastore image.
  ssh:
    logLevel: "INFO"

  # -- Liveness probe configuration.
  # @default -- This object has the following default values for the liveness probe configuration.
  livenessProbe:
    # -- Enable liveness probe.
    enabled: true
    httpGet:
      # -- HTTP path for liveness probe.
      path: /v1/health
      # -- Port for liveness probe.
      port: http
    # -- Initial delay before liveness probe is initiated.
    initialDelaySeconds: 10
    # -- Timeout for liveness probe.
    timeoutSeconds: 5
    # -- Period for liveness probe.
    periodSeconds: 30
    # -- Success threshold for liveness probe.
    successThreshold: 1
    # -- Failure threshold for liveness probe.
    failureThreshold: 20

  # -- Readiness probe configuration.
  # @default -- This object has the following default values for the readiness probe configuration.
  readinessProbe:
    # -- Enable readiness probe.
    enabled: true
    httpGet:
      # -- HTTP path for readiness probe.
      path: /v1/health
      # -- Port for readiness probe.
      port: http
    # -- Initial delay before readiness probe is initiated.
    initialDelaySeconds: 30
    # -- Timeout for readiness probe.
    timeoutSeconds: 30
    # -- Period for readiness probe.
    periodSeconds: 20
    # -- Success threshold for readiness probe.
    successThreshold: 1
    # -- Failure threshold for readiness probe.
    failureThreshold: 40

  # -- Start-up probe configuration.
  # @default -- This object has the following default values for the start-up probe configuration.
  startupProbe:
    # -- Enable start-up probe.
    enabled: false
    # -- TCP socket configuration for start-up probe.
    tcpSocket:
      port: http
    # -- Initial delay before start-up probe is initiated.
    initialDelaySeconds: 60
    # -- Timeout for start-up probe.
    timeoutSeconds: 1
    # -- Period for start-up probe.
    periodSeconds: 10
    # -- Success threshold for start-up probe.
    successThreshold: 1
    # -- Failure threshold for start-up probe.
    failureThreshold: 10

  # -- Redis cluster configuration.
  # @default -- This object has the following default values for the Redis cluster configuration.
  redis-cluster:
    # -- Enable Redis cluster.
    enabled: false
    # -- Whether to use password authentication.
    usePassword: false
    # -- Number of redis cluster master nodes.
    cluster:
      nodes: 3 # default: 6
      replicas: 0 # default: 1

  # -- PostgreSQL high availability (HA) configuration.
  # @default -- This object has the following default values for the PostgreSQL HA configuration.
  postgresql-ha:
    global:
      postgresql:
        # -- Global PostgreSQL database name.
        database: datastore
        # -- Global password for the `datastore` user.
        password: datastore
        # -- Global username for the `datastore` user.
        username: datastore
    # -- Enable PostgreSQL HA. If enabled, configures PostgreSQL HA using the [bitnami/postgresql-ha](https://github.com/bitnami/charts/tree/main/bitnami/postgresql-ha) chart.
    enabled: false
    postgresql:
      # -- Repmgr password for the `datastore` user.
      repmgrPassword: changeme2
      # -- Postgres password for the `datastore` user.
      postgresPassword: changeme1
      # -- Password for the `datastore` user.
      password: changeme4
    pgpool:
      # -- Pgpool admin password.
      adminPassword: changeme3
    service:
      # -- PostgreSQL service port.
      ports:
        postgresql: 5432
    primary:
      # -- PVC storage request for PostgreSQL HA volume.
      persistence:
        size: 10Gi

  postgresql:
    # -- Enable or disable the built-in PostgreSQL database.
    enabled: true
    global:
      postgresql:
        auth:
          # -- Password for the datastore database user.
          password: datastore
          # -- Name of the database to create.
          database: datastore
          # -- Username for the database user.
          username: datastore
        service:
          ports:
            # -- Port number for PostgreSQL service.
            postgresql: 5432
    primary:
      persistence:
        # -- Storage size request for the PostgreSQL persistent volume.
        size: 10Gi

  test:
    # -- Enable or disable the test-connection Pod.
    enabled: false
    image:
      # -- Image name for the wget container used in the test-connection Pod.
      name: busybox
      # -- Image tag for the wget container used in the test-connection Pod.
      tag: latest

  # -- Set to false to skip the basic validation check.
  checkDeprecation: true

  # -- Array of extra objects to deploy with the release.
  extraDeploy: []

  # -- Whether to serve traffic directly from an object storage service.
  serveDirect: true

  # -- Object Store configuration settings for accessing external Object Storage.
  # @default -- This object has the following default values for the object store configuration.
  objectStore:
    # -- Enable or disable object storage integration.
    enabled: true
    # -- Object storage service endpoint URL.
    endpoint: "c8j7.par5.idrivee2-11.com"
    # -- Access key credential for object storage authentication.
    accessKey: "MvNGzmLuvoga36Kk6UbF"
    # -- Secret key credential for object storage authentication.
    accessSecret: "zocpC6PF4QwFOViGCLCekdQRDckVCE6qYEYfVHoo"
    # -- Name of the bucket to use for object storage.
    bucketName: "hc-datasets"
    # -- Geographic region for the object storage service.
    region: "paris"
    # -- Enable or disable SSL/TLS for object storage connections.
    ssl: true
    # -- Name of existing Kubernetes secret containing object storage credentials.
    existingSecret: ""
    # -- Key in existing secret that contains the access key.
    existingSecretAccessKey: ""
    # -- Key in existing secret that contains the secret key.
    existingSecretAccessSecret: ""

  # -- External URL configuration for the NeMo Data Store microservice.
  external:
    # -- The external URL where users will access the NeMo Data Store microservice.
    rootUrl: ""
    # -- The external URL's domain name.
    domain: ""

  # -- External PostgreSQL configuration settings. These values are only used when postgresql.enabled is set to false.
  # @default -- This object has the following default values for the external PostgreSQL configuration.
  externalDatabase:
    # -- External database host address.
    host: ""
    # -- External database port number.
    port: 5432
    # -- Database username for Datastore service.
    user: ""
    # -- Datastore database name.
    database: ""
    # -- Name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- Name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    # -- SSL mode for external database connection.
    sslMode: "disable"

  # -- JWT secret configuration settings.
  # @default -- This object has the following default values for the JWT secret configuration.
  jwtSecret:
    # -- User specified LFS JWT secret - this will be stored in a secret.
    value: ""
    # -- Name of an existing secret resource containing the LFS JWT secret.
    existingSecret: ""
    # -- Key in existing secret containing the LFS JWT secret.
    existingSecretKey: ""


  # -- Parameters for the demo mode.
  # @default -- This object has the following default values for the demo parameters.
  demo:
    # -- Enable or disable the demo mode.
    enabled: false
    # -- Name of the secret containing the NGC image pull secret.
    ngcImagePullSecret: ngc-image-pull-secret
    # -- NGC API key.
    ngcApiKey: ""

  # -- The service name for the NeMo Data Store microservice.
  serviceName: nemo-data-store


customizer:
  # -- String to override chart name on resulting objects when deployed.
  nameOverride: ""
  # -- String to fully override the chart and release name on resulting objects when deployed.
  fullnameOverride: ""

  # -- NeMo Customizer image that supports training and standalone mode.
  # @default -- This object has the following default values for the NeMo Customizer microservice image.
  image:
    # -- Registry for the NeMo Customizer image.
    registry: nvcr.io
    # -- Repository for the NeMo Customizer image.
    repository: nvidia/nemo-microservices/customizer
    # -- Image pull policy for the NeMo Customizer image.
    imagePullPolicy: IfNotPresent

  # -- NeMo Customizer image that supports training ONLY for gpt-oss model
  # @default -- This object has the following default values for the NeMo Customizer microservice image.
  gptOssImage:
    # -- Registry for the NeMo Customizer image.
    registry: nvcr.io
    # -- Repository for the NeMo Customizer image.
    repository: nvidia/nemo-microservices/customizer-gpt-oss
    # -- Image pull policy for the NeMo Customizer image.
    imagePullPolicy: IfNotPresent

  # -- Customizer API only image configuration.
  # @default -- This object has the following default values for the NeMo Customizer API only image.
  apiImage:
    # -- Registry for the NeMo Customizer API image.
    registry: nvcr.io
    # -- Repository for the NeMo Customizer API image.
    repository: nvidia/nemo-microservices/customizer-api
    # -- Image pull policy for the NeMo Customizer API image.
    imagePullPolicy: IfNotPresent

  # -- Image pull secrets configuration.
  imagePullSecrets:
    - name: nvcrimagepullsecret

  # -- Download models to PVC model cache configuration.
  # @default -- This object has the following default values for the model downloader.
  modelDownloader:
    # -- Security context for the model downloader.
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      runAsGroup: 1000
    # -- Time to live in seconds after the job finishes.
    ttlSecondsAfterFinished: 1800

  # -- Secret used for auto hydrating the model cache from NGC for enabled models.
  ngcAPISecret: "ngc-api"
  # -- Key in the NGC API secret containing the API key.
  ngcAPISecretKey: "NGC_API_KEY"
  # -- The K8s Secret containing the HuggingFace API token.
  hfAPISecret: "hf-token"
  # -- The key in the hfAPISecret containing the actual secret's value. Defaults to HF_TOKEN
  hfAPISecretKey: "HF_TOKEN"


  # -- A map of environment variables to inject into the NeMo Customizer app container.
  # Example:
  #
  # `{HOST_IP:
  #   valueFrom:
  #     fieldRef:
  #       fieldPath: status.hostIP
  # OTEL_EXPORTER_OTLP_ENDPOINT: "http://$(HOST_IP):4317"}`
  env:
    # How long to wait until automatically deleting the NemoTrainingJob
    JOB_CLEANUP_TTL_SEC: "3600"
    # How often in seconds to poll for status conditions from NemoTrainingJob
    JOB_STATUS_POLLING_INTERVAL: "15"
    # How often in seconds to poll for events and logs for all objects owned by NemoTrainingJob, including
    # volcano/runai jobs, pods. This is longer because it is a more expensive IO operation.
    JOB_LOGS_POLLING_INTERVAL: "600"
    # Given a failure in the watch loop, how long to sleep in secs before iterating a new loop.
    TRAINING_JOB_CREATION_FAILURE_SLEEP_INTERVAL: "15"
    # How many failures to tolerate in the NemoTrainingJob watcher loop before terminating the loop and the job itself.
    MAX_TRAINING_JOB_STATUS_FAILURES: 10

  readinessProbe:
    initialDelaySeconds: 30
    timeoutSeconds: 15
    failureThreshold: 15

  livenessProbe:
    initialDelaySeconds: 30
    timeoutSeconds: 15
    failureThreshold: 15

  # -- Tools configuration for downloading and uploading entities to NeMo Data Store.
  # @default -- This object has the following default values for the NeMo Data Store tools image.
  nemoDataStoreTools:
    # -- Registry for the NeMo Data Store tools image.
    registry: nvcr.io
    # -- Repository for the NeMo Data Store tools image.
    repository: nvidia/nemo-microservices/nds-v2-huggingface-cli
    # -- Tag for the NeMo Data Store tools image.
    tag: ""
    # -- Image pull secret for the NeMo Data Store tools image.
    imagePullSecret: nvcrimagepullsecret

  # -- Service configuration.
  service:
    # -- Type of Kubernetes service to create.
    type: ClusterIP
    # -- External port for the service.
    port: 8000
    # -- Internal port for the service.
    internalPort: 9009

  # -- Number of replicas to deploy.
  replicaCount: 1

  # -- Service account configuration.
  serviceAccount:
    # -- Specifies whether a service account should be created.
    create: true
    # -- The name of the service account to use. If not set and create is true, a name is generated.
    name: ""
    # -- Annotations to add to the service account.
    annotations: {}
    # -- Automatically mount a ServiceAccount's API credentials.
    automountServiceAccountToken: true


  # -- Configure the PVC for models mount, where we store the parent/base models.
  modelsStorage:
    # -- Enable persistent volume for model storage.
    enabled: true
    # -- Storage class name for the models PVC. Empty string uses the default storage class.
    storageClassName: "retain-single"
    # -- Size of the persistent volume.
    size: 1024Gi
    # -- Access modes for the persistent volume.
    accessModes:
      - ReadWriteMany

  # -- Logging configuration.
  logging:
    # -- Log level for the application.
    logLevel: INFO
    # -- Enable logging for health endpoints.
    logHealthEndpoints: false

  # -- Configuration for the NeMo Customizer microservice.
  # @default -- This object has default values for the following fields.
  customizerConfig:
    # -- Specifies the internal K8s DNS record for the NeMo Data Store service.
    nemoDataStoreURL: "http://nemo-data-store:3000"
    # -- Specifies the internal K8s DNS record for the NeMo Entity Store service.
    entityStoreURL: "http://nemo-entity-store:8000"
    # -- URL for the MLflow tracking server.
    mlflowURL: "" # "http://mlflow-tracking.mlflow-system.svc.cluster.local:80"

    # -- Weights and Biases (WandB) Python SDK intialization configuration for logging and monitoring training jobs in WandB.
    wandb:
      # -- The username or team name under which the runs will be logged.
      # -- If not specified, the run will default to a default entity set in the account settings.
      # -- To change the default entity, go to the account settings https://wandb.ai/settings
      # -- and update the "Default location to create new projects" under "Default team".
      # -- Reference: https://docs.wandb.ai/ref/python/init/
      entity: pamanseau
      # The name of the project under which this run will be logged
      project: "nvidia-nemo-customizer"

    # -- Network configuration for training jobs on Oracle Kubernetes Engine (OKE) on Oracle Cloud Infrastructure (OCI).
    trainingNetworking:
      - name: NCCL_IB_SL
        value: 0
      - name: NCCL_IB_TC
        value: 41
      - name: NCCL_IB_QPS_PER_CONNECTION
        value: 4
      - name: UCX_TLS
        value: TCP
      - name: UCX_NET_DEVICES
        value: eth0
      - name: HCOLL_ENABLE_MCAST_ALL
        value: 0
      - name: NCCL_IB_GID_INDEX
        value: 3

    # -- Training configuration for customization jobs.
    # @default -- This object has the following default values for the training configuration.
    training:
      # -- Queue name used by the underlying scheduler of NemoTrainingJob.
      # Maps to "resourceGroup" in NemoTrainingJob.
      queue: "default"

      # -- Directory path for training workspace.
      workspace_dir: "/pvc/workspace"

      # -- Training timeout in seconds. If job times out, it will be marked as failed and no checkpoints are saved.
      # training_timeout: 3600

      # -- Interval in seconds to poll for monitoring jobs.
      # Defaults to 10s. poll_interval_seconds with a 30 second pad must be less than ttl_seconds_after_finished.
      poll_interval_seconds: 10

      # -- Time to live in seconds after the training job pod completes.
      # Defaults to 1h. Take precautions when setting ttl_seconds_after_finished to 0 which disables automatic clean up for
      # jobs. When disabled, jobs will persist and hold on to resources like PVCs and will require manual or external clean up.
      # ttl_seconds_after_finished must be greater than poll_interval_seconds with a 30 second pad.
      ttl_seconds_after_finished: 3600

      # container_defaults lets you configure the training container similar to a K8s object.
      # Currently, it only supports a subset of the K8s Container Spec.
      # Note it is named _defaults because we plan to allow training specific sections in the future that can override this section.
      # -- Default container configuration for training jobs.
      container_defaults:
        # env holds a list of custom environment variables injected into the training container.
        # However, they cannot override env variables reserved by Customizer. The application validates this at start time.
        # An example use case is configuring OpenTelemetry in the training container.
        # By default, the training container inherits its OpenTelemetry environment values from the openTelemetry section.
        # However, by setting the OpenTelemetry env variables here, the user can override the behavior set in the openTelemetry section.
        # -- Environment variables for the training container.
        # Cannot override env variables reserved by NeMo Customizer.
        env:
          # - name: HOST_IP
          #   valueFrom:
          #     fieldRef:
          #       fieldPath: "status.hostIP"
          # - name: NAMESPACE
          #   valueFrom:
          #     fieldRef:
          #       fieldPath: "metadata.namespace"
          # - name: OTEL_RESOURCE_ATTRIBUTES
          #   value: "deployment.environment=$(NAMESPACE)"
          # - name: OTEL_EXPORTER_OTLP_ENDPOINT
          #   value: "http://$(HOST_IP):4317"
          # - name: OTEL_TRACES_EXPORTER
          #   value: otlp
          # - name: OTEL_METRICS_EXPORTER
          #   value: otlp
          # - name: OTEL_LOGS_EXPORTER
          #   value: none
        imagePullPolicy: IfNotPresent
      # PVC config for NemoTrainingJob, which automatically creates one for each job
      # This is implicitly mounted at /pvc to our training container
      pvc:
        # -- Storage class for the training job PVC.
        storageClass: ""
        # -- Size of the training job PVC.
        size: 5Gi
        # -- Volume access mode for the training job PVC.
        volumeAccessMode: "ReadWriteOnce"
        # -- The name of a single PVC to be used for training. If null, create a separate PVC per training job for isolation. If provided, it will create this PVC.
        name: null

    # -- OpenTelemetry settings.
    # @default -- This object has the following default values for the OpenTelemetry settings.
    openTelemetry:
      # -- Whether to enable OpenTelemetry.
      enabled: true
      # -- Sets the traces exporter type (otlp, console, none).
      tracesExporter: otlp
      # -- Sets the metrics exporter type (otlp, console, none).
      metricsExporter: otlp
      # -- Sets the logs exporter type (otlp, console, none).
      logsExporter: otlp
      # -- Endpoint to access a custom OTLP collector listening on port 4317. Example: "http://$(HOST_IP):4317".
      exporterOtlpEndpoint: ""

    # -- Tolerations on the customization job pods.
    tolerations: []

  # -- PostgreSQL configuration for the NeMo Customizer microservice.
  # @default -- This object has the following default values for the PostgreSQL configuration.
  postgresql:
    # -- Whether to enable or disable the PostgreSQL helm chart.
    enabled: true
    # -- The name override for the Customizer PostgreSQL database.
    nameOverride: customizerdb
    auth:
      # -- Whether to assign a password to the "postgres" admin user. Otherwise, remote access will be blocked for this user.
      enablePostgresUser: true
      # -- Name for a custom user to create.
      username: nemo
      # -- Password for the custom user to create.
      password: nemo
      # -- Name for a custom database to create.
      database: finetuning
      # -- Name of existing secret to use for PostgreSQL credentials.
      existingSecret: ""
    # -- PostgreSQL architecture (`standalone` or `replication`).
    architecture: standalone
    serviceAccount:
      name: customizer-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true

  # -- External PostgreSQL configuration.
  # @default -- This object has the following default values for the external PostgreSQL configuration.
  externalDatabase:
    # -- External database host address.
    host: localhost
    # -- External database port number.
    port: 5432
    # -- Database username for the NeMo Customizer microservice.
    user: nemo
    # -- Name of the database to use.
    database: finetuning
    # -- Name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- Name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    uriSecret:
      name: ""
      key: ""

  # -- WandB configuration.
  # @default -- This object has the following default values for the WandB configuration.
  wandb:
    # -- WandB secret value. Must contain exactly 32 alphanumeric characters. Creates a new Kubernetes secret named "wandb-secret" with key-value pair "encryption_key=<wandb.secretValue>". Ignored if wandb.existingSecret is set.
    secretValue: ec60d96b639764ccf9859bc10d4363d1
    # -- Name of an existing Kubernetes secret resource for the WandB encryption secret.
    existingSecret: ""
    # -- Name of the key in the existing WandB secret containing the secret value. The secret value must be exactly 32 alphanumeric characters: ^[a-zA-Z0-9]{32}$
    existingSecretKey: ""

  # -- Ingress configuration.
  # @default -- This object has the following default values for the Ingress configuration.
  ingress:
    # -- Whether to enable the ingress resource.
    enabled: false
    # -- Ingress class name.
    className: ""
    # -- Additional annotations for the Ingress resource.
    annotations: {}
    # -- Hostname for the ingress resource.
    hostname: ""
    # -- (list) A list of maps, each containing the keys `host` and `paths` for the ingress resource. You must specify a list for configuring ingress for the microservice.
    # @default -- []
    hosts: {}
      # - host: ""
      #   paths:
      #     - path: /
      #       pathType: ImplementationSpecific
    # -- TLS configuration for the ingress resource.
    tls: []

  # -- Open Telemetry Collector configuration.
  # @default -- This object has the following default values for the Open Telemetry Collector configuration.
  opentelemetry-collector:
    # -- Switch to enable or disable Open Telemetry Collector.
    enabled: true
    image:
      # -- Repository for Open Telemetry Collector image.
      repository: "otel/opentelemetry-collector-k8s"
      # -- Overrides the image tag whose default is the chart appVersion.
      tag: "0.102.1"
    # -- Deployment mode for Open Telemetry Collector. Valid values are "daemonset", "deployment", and "statefulset".
    mode: deployment
    # -- Base collector configuration for Open Telemetry Collector.
    config:
      receivers:
        otlp:
          protocols:
            grpc: {}
            http:
              cors:
                allowed_origins:
                  - "*"
      exporters:
        debug:
          verbosity: detailed
      extensions:
        health_check: {}
        zpages:
          endpoint: 0.0.0.0:55679
      processors:
        batch: {}
      service:
        extensions: [zpages, health_check]
        pipelines:
          traces:
            receivers: [otlp]
            exporters: [debug]
            processors: [batch]
          metrics:
            receivers: [otlp]
            exporters: [debug]
            processors: [batch]
          logs:
            receivers: [otlp]
            exporters: [debug]
            processors: [batch]

  # -- Enable or disable RunAI executor.
  useRunAIExecutor: false

  # -- Deployment configurations for AWS
  awsDeploy:
    # -- Switch on if using AWS and kyverno is installed
    enabled: false
    # -- EFA number of devices per GPU
    efaDevicesPerGPU: 4
  # -- List of model configurations supported by the Customizer.
  # @default -- This object has the following default values.
  customizationTargets:
    # Allow model downloads from Hugging Face
    hfTargetDownload:
      # -- set this to true to allow model downloads from Hugging Face. If enabled=false, models are not allwed to be downloaded from any Hugging Face org and allowedHfOrgs is disregarded
      enabled: true
      # -- List of allowed organizations for model downloads from Hugging Face. Empty list allows all organizations.
      # Example:
      # allowedHfOrgs:
      #   - "nvidia"
      allowedHfOrgs:
        - "nvidia"
        - "meta"
        - "microsoft"
        - "openai"

    # -- Whether to have this values file override targets in the database on application start
    overrideExistingTargets: true
    # -- The default targets to populate the database with
    # @default -- This object has the following default values.
    targets:
      # -- Llama 3.2 1B target model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B model.
      meta/llama-3.2-1b@2.0:
      # -- The name for target model.
        name: llama-3.2-1b@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.2 1B model.
        model_uri: ngc://nvidia/nemo/llama-3_2-1b:2.0
        # -- Path where model files are stored.
        model_path: llama32_1b_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.2-1b
        # -- Number of model parameters.
        num_parameters: 1000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.2 1B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B Instruct model.
      meta/llama-3.2-1b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.2-1b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: true
        # -- NGC model URI
        model_uri: ngc://nvidia/nemo/llama-3_2-1b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama32_1b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.2-1b-instruct
        # -- Number of model parameters.
        num_parameters: 1000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.2 3B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.2 3B Instruct model.
      meta/llama-3.2-3b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.2-3b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI.
        model_uri: ngc://nvidia/nemo/llama-3_2-3b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama32_3b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.2-3b-instruct
        # -- Number of model parameters.
        num_parameters: 3000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3 70B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3 70B Instruct model.
      meta/llama3-70b-instruct@2.0:
        # -- The name for target model.
        name: llama3-70b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3-70b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3-70b-bf16_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama3-70b-instruct
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.1 8B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.1 8B Instruct model.
      meta/llama-3.1-8b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.1-8b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: true
        # -- NGC model URI for Llama 3.1 8B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_1-8b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3_1-8b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.1-8b-instruct
        # -- Number of model parameters.
        num_parameters: 8000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.1 70B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.1 70B Instruct model.
      meta/llama-3.1-70b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.1-70b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.1 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_1-70b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3_1-70b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.1-70b-instruct
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Phi-4 model target configuration.
      # @default -- This object has the following default values for the Phi-4.
      microsoft/phi-4@1.0:
        # -- The name for target model.
        name: phi-4@1.0
        # -- The namespace for target model.
        namespace: microsoft
        # -- The version for target model.
        version: "1.0"
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Phi-4 model.
        model_uri: ngc://nvidia/nemo/phi-4:1.0
        # -- Path where model files are stored.
        model_path: phi-4_1_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: microsoft/phi-4
        # -- Number of model parameters.
        num_parameters: 14659507200
        # -- Model precision format.
        precision: bf16

      # -- Llama 3.3 70B Instruct model target configuration.
      # @default -- This object has the following default values for the Llama 3.3 70B Instruct model.
      meta/llama-3.3-70b-instruct@2.0:
        # -- The name for target model.
        name: llama-3.3-70b-instruct@2.0
        # -- The namespace for target model.
        namespace: meta
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.3 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_3-70b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama-3_3-70b-instruct_2_0
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: meta/llama-3.3-70b-instruct
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Nemotron Nano Llama 3.1 8B Instruct model target configuration.
      # @default -- This object has the following default values for the Nemotron Nano Llama 3.1 8B Instruct model.
      nvidia/nemotron-nano-llama-3.1-8b@1.0:
        # -- The name for target model.
        name: nemotron-nano-llama-3.1-8b@1.0
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI
        model_uri: ngc://nvidia/nemo/nemotron-nano-3_1-8b:0.0.1
        # -- Path where model files are stored.
        model_path: nemotron-nano-3_1-8b_0_0_1
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: nvidia/nemotron-nano-llama-3.1-8b
        # -- Number of model parameters.
        num_parameters: 8000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Nemotron Super Llama 3.3 49B Instruct model target configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@1.0:
        # -- The name for target model.
        name: nemotron-super-llama-3.3-49b@1.0
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI
        model_uri: ngc://nvidia/nemo/nemotron-super-3_3-49b:v1
        # -- Path where model files are stored.
        model_path: nemotron-super-3_3-49b_v1
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: nvidia/nemotron-super-llama-3.3-49b
        # -- Number of model parameters.
        num_parameters: 4900000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Nemotron Super Llama 3.3 49B v1.5 Instruct model target configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B v1.5 Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@1.5:
        # -- The name for target model.
        name: nemotron-super-llama-3.3-49b@1.5
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI
        model_uri: hf://nvidia/Llama-3_3-Nemotron-Super-49B-v1_5
        # -- Endpoint for where to find this model
        hf_endpoint: https://huggingface.co
        # -- Path where model files are stored.
        model_path: nemotron-super-3_3-49b_v1_5
        # -- Mapping to the model name in NIM and Recipe Selection in Customizer. Defaults to being the same as the the configuration entry namespace/name.
        base_model: nvidia/nemotron-super-llama-3.3-49b-v1.5
        # -- Number of model parameters.
        num_parameters: 4900000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- Llama 3.2 1B embedding target model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B embedding model.
      nvidia/llama-3.2-nv-embedqa-1b@v2:
        # -- The name for target model.
        name: llama-3.2-nv-embedqa-1b@v2
        # -- The namespace for target model.
        namespace: nvidia
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI
        model_uri: ngc://nvidia/nemo/llama-3_2-1b-embedding-base:0.0.1
        # -- Path where model files are stored.
        model_path: llama32_1b-embedding
        # -- Mapping to the model name to the optimized llama embedding training script and NIM
        base_model: nvidia/llama-3.2-nv-embedqa-1b-v2
        # -- Number of model parameters.
        num_parameters: 1000000000
        # -- Model precision format.
        precision: bf16-mixed

      # -- openai/gpt-oss-20b target model configuration.
      # @default -- This object has the following default values for the openai/gpt-oss-20b model.
      openai/gpt-oss-20b@v1:
        # -- The name for target model.
        name: gpt-oss-20b@v1
        # -- The namespace for target model.
        namespace: openai
        # -- Whether to enable the model.
        enabled: false
        # -- HF model URI
        model_uri: hf://openai/gpt-oss-20b
        # -- HF endpoint
        hf_endpoint: https://huggingface.co
        # -- Path where model files are stored.
        model_path: gpt_oss_20b-embedding
        # -- Mapping to the model name to the optimized llama embedding training script and NIM
        base_model: openai/gpt-oss-20b
        # -- Number of model parameters.
        num_parameters: 21000000000
        # -- Model precision format.
        precision: bf16-mixed

  # -- List of customization configuration template supported by the Customizer.
  # @default -- This object has the following default values.
  customizationConfigTemplates:
    # -- Whether to have this values file override templates in the database on application start
    overrideExistingTemplates: true
    # -- The default templates to populate the database with
    # @default -- This object has the following default values.
    templates:
      # -- Llama 3.2 3B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 3B Instruct model.
      meta/llama-3.2-3b-instruct@v1.0.0+A100:
        # -- The name for training config template.
        name: llama-3.2-3b-instruct@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-3b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.2 1B model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B model.
      meta/llama-3.2-1b@v1.0.0+A100:
        # -- The name for training config template.
        name: llama-3.2-1b@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-1b@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
              # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.2 1B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B Instruct model.
      meta/llama-3.2-1b-instruct@v1.0.0+A100:
        # -- The name for training config template.
        name: llama-3.2-1b-instruct@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-1b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
              # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: dpo
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3 70B Instruct model.
      meta/llama3-70b-instruct@v1.0.0+A100:
        # -- The name for training config template.
        name: llama3-70b-instruct@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama3-70b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 8B Instruct model.
      meta/llama-3.1-8b-instruct@v1.0.0+A100:
        # -- The name for training config template.
        name: llama-3.1-8b-instruct@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama-3.1-8b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
              # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: dpo
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 8
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 70B Instruct model.
      meta/llama-3.1-70b-instruct@v1.0.0+A100:
        # -- The name for training config template.
        name: llama-3.1-70b-instruct@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama-3.1-70b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Phi-4 model configuration.
      # @default -- This object has the following default values for the Phi-4.
      microsoft/phi-4@v1.0.0+A100:
        # -- The name for training config template.
        name: phi-4@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: microsoft
        # -- The target to perform the customization on.
        target: microsoft/phi-4@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.3 70B Instruct model.
      meta/llama-3.3-70b-instruct@v1.0.0+A100:
        # -- The name for training config template.
        name: llama-3.3-70b-instruct@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.3-70b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Nano Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Nano Llama 3.1 8B Instruct model.
      nvidia/nemotron-nano-llama-3.1-8b@v1.0.0+A100:
        # -- The name for training config template.
        name: nemotron-nano-llama-3.1-8b@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-nano-llama-3.1-8b@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      nvidia/llama-3.2-nv-embedqa-1b@v2+A100:
        # -- The name for training config template.
        name: llama-3.2-nv-embedqa-1b@v2+A100
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/llama-3.2-nv-embedqa-1b@v2
        # -- Resource configuration for each training option for the target model.
        training_options:
        # -- Training method.
        # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1

        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 2048
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Super Llama 3.3 49B Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@v1.0.0+A100:
        # -- The name for training config template.
        name: nemotron-super-llama-3.3-49b@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-super-llama-3.3-49b@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Super Llama 3.3 49B v1.5 Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B v1.5 Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@v1.5+A100:
        # -- The name for training config template.
        name: nemotron-super-llama-3.3-49b@v1.5+A100
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-super-llama-3.3-49b@1.5
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"


      #### ------------------------------ Nvidia L40 GPU default configurations ------------------------------ ####
      # -- Llama 3.2 1B model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B model.
      meta/llama-3.2-1b@v1.0.0+L40:
        # -- The name for training config template.
        name: llama-3.2-1b@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-1b@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
            # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.2 1B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B Instruct model.
      meta/llama-3.2-1b-instruct@v1.0.0+L40:
        # -- The name for training config template.
        name: llama-3.2-1b-instruct@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-1b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
            # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: dpo
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 2
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      nvidia/llama-3.2-nv-embedqa-1b@v2+L40:
        # -- The name for training config template.
        name: llama-3.2-nv-embedqa-1b@v2+L40
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/llama-3.2-nv-embedqa-1b@v2
        # -- Resource configuration for each training option for the target model.
        training_options:
        # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 2048

      # -- Llama 3.2 3B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 3B Instruct model.
      meta/llama-3.2-3b-instruct@v1.0.0+L40:
        # -- The name for training config template.
        name: llama-3.2-3b-instruct@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.2-3b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 1
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 8B Instruct model.
      meta/llama-3.1-8b-instruct@v1.0.0+L40:
        # -- The name for training config template.
        name: llama-3.1-8b-instruct@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama-3.1-8b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
            # -- Training method.1
          - training_type: sft
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          - training_type: dpo
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 8
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"


      # -- Llama 3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3 70B Instruct model.
      meta/llama3-70b-instruct@v1.0.0+L40:
        # -- The name for training config template.
        name: llama3-70b-instruct@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama3-70b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 70B Instruct model.
      meta/llama-3.1-70b-instruct@v1.0.0+L40:
        # -- The name for training config template.
        name: llama-3.1-70b-instruct@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: meta
        # -- Resource configuration for each training option for the target model.
        target: meta/llama-3.1-70b-instruct@2.0
        # -- Training options for different fine-tuning methods.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"


      # -- Llama 3.3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.3 70B Instruct model.
      meta/llama-3.3-70b-instruct@v1.0.0+L40:
        # -- The name for training config template.
        name: llama-3.3-70b-instruct@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: meta
        # -- The target to perform the customization on.
        target: meta/llama-3.3-70b-instruct@2.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Nano Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Nano Llama 3.1 8B Instruct model.
      nvidia/nemotron-nano-llama-3.1-8b@v1.0.0+L40:
        # -- The name for training config template.
        name: nemotron-nano-llama-3.1-8b@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-nano-llama-3.1-8b@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 2
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1

        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Nemotron Super Llama 3.3 49B Instruct model configuration.
      # @default -- This object has the following default values for the Nemotron Super Llama 3.3 49B Instruct model.
      nvidia/nemotron-super-llama-3.3-49b@v1.0.0+L40:
        # -- The name for training config template.
        name: nemotron-super-llama-3.3-49b@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: nvidia
        # -- The target to perform the customization on.
        target: nvidia/nemotron-super-llama-3.3-49b@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 4
            # -- The number of nodes to use for the specified training.
            num_nodes: 2
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 4
            # -- The number of GPUs among which the models layers are distributed.
            pipeline_parallel_size: 2
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Phi-4 model configuration.
      # @default -- This object has the following default values for the Phi-4.
      microsoft/phi-4@v1.0.0+L40:
        # -- The name for training config template.
        name: phi-4@v1.0.0+L40
        # -- The namespace for training config template.
        namespace: microsoft
        # -- The target to perform the customization on.
        target: microsoft/phi-4@1.0
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: lora
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 2
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 4096
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

      # -- Openai gpt-oss 20B model configuration.
      # @default -- This object has the following default values for the OpenAI gpt-oss 20b model.
      openai/gpt-oss-20b@v1.0.0+A100:
        # -- The name for training config template.
        name: gpt-oss-20b@v1.0.0+A100
        # -- The namespace for training config template.
        namespace: openai
        # -- The target to perform the customization on.
        target: openai/gpt-oss-20b@v1
        # -- Resource configuration for each training option for the target model.
        training_options:
          # -- Training method.
          - training_type: sft
            # -- The type of fine-tuning method.
            finetuning_type: all_weights
            # -- The number of GPUs per node to use for the specified training.
            num_gpus: 8
            # -- The number of nodes to use for the specified training.
            num_nodes: 1
            # -- The number of GPUs among which the models tensors are partitioned.
            tensor_parallel_size: 1
            # -- Number of GPUs used to split the model across layers for pipeline model.
            pipeline_parallel_size: 2
            # -- Number of GPUs used to parallelize expert (MoE) components of the model.
            expert_model_parallel_size: 4
            # -- The number of training examples processed in parallel by each individual GPU.
            micro_batch_size: 1
        # -- The largest context used for training. Datasets are truncated based on the maximum sequence length.
        max_seq_length: 2048
        # -- Prompt template used to extract keys from the dataset.
        prompt_template: "{prompt} {completion}"

  # -- Sets predictable name for the NeMo Customizer Kubernetes service
  serviceName: nemo-customizer


nim:
  # -- Specifies whether to deploy a NIM for LLM during the Helm installation of the chart. You can deploy a single static NIM by enabling this object and its values. When enabled, the chart deploys `meta/llama-3.1-8b-instruct` as the default NIM.
  enabled: false
  image:
    # Adjust to the actual location of the image and version you want
    repository: nvcr.io/nim/meta/llama-3.1-8b-instruct
    tag: "1.8"
  imagePullSecrets:
    - name: nvcrimagepullsecret
  model:
    # -- The name of the model to deploy as NIM.
    name: meta/llama-3.1-8b-instruct # not strictly necessary, but enables running "helm test"
    # -- The NGC API secret for model access.
    ngcAPISecret: ngc-api
  service:
    # -- Labels for the NIM service.
    labels:
      app.nvidia.com/nim-type: inference
  # -- Environment variables for the NIM service.
  env:
    - name: NIM_PEFT_SOURCE
      value: http://nemo-entity-store:8000
    - name: NIM_PEFT_REFRESH_INTERVAL
      value: "30"
    - name: NIM_MAX_CPU_LORAS
      value: "16"
    - name: NIM_MAX_GPU_LORAS
      value: "8"
  persistence:
    # -- Specifies whether to enable persistence volume claim (PVC) for the NIM service.
    enabled: true
    # -- Specifies the storage class to use for the PVC.
    storageClass: ""
    # -- Annotations for the PVC.
    annotations:
      helm.sh/resource-policy: keep
  statefulSet:
    # -- Specifies whether to enable a stateful set for the NIM service.
    enabled: false
  # -- Specifies resources for the NIM service.
  resources:
    limits:
      nvidia.com/gpu: 1
    # -- Specifies requests for the NIM service.
    requests:
      nvidia.com/gpu: 1

evaluator:
  # -- The number of replicas for the NeMo Evaluator microservice.
  replicaCount: 1

  image:
    # -- The image pull policy for the NeMo Evaluator microservice image.
    pullPolicy: IfNotPresent
    # -- The repository where the NeMo Evaluator microservice image is located.
    repository: nvcr.io/nvidia/nemo-microservices/evaluator
    # -- Specifies the version of the NeMo Evaluator microservice image.
    tag: ""

  # -- The image pull secrets for accessing the container registry.
  imagePullSecrets:
    - name: nvcrimagepullsecret

  # -- The name override for the NeMo Evaluator microservice.
  nameOverride: ""

  # -- The full name override for the NeMo Evaluator microservice.
  fullnameOverride: ""

  serviceAccount:
    # -- Whether to create a service account for the NeMo Evaluator microservice.
    create: true
    # -- Whether to automatically mount the service account token.
    automount: true
    # -- Annotations for the service account.
    annotations: {}
    # -- A name for the service account.
    name: ""

  # -- Annotations for the service pod.
  podAnnotations: {}
  # -- Labels for the service pod.
  podLabels: {}

  # -- Security context for the service pod.
  podSecurityContext: {}

  # -- Security context for the service container.
  securityContext: {}

  # -- Additional environment variables to pass to containers. The format is `NAME: value` or `NAME: valueFrom: {object}`.
  env:
    {}

  service:
    # -- The type of the NeMo Evaluator microservice.
    type: ClusterIP
    # -- External port of the NeMo Evaluator microservice.
    port: 7331
    # -- Internal port of the NeMo Evaluator microservice.
    internalPort: 7332

  evaluationJob:
    # -- Monitoring interval checking evaluation job status (in seconds).
    monitoringInterval: 5
    # -- Monitoring timeout for checking evaluation job status (in seconds).
    monitoringTimeout: 36000

  evalFactory:
    job:
      # -- Restart policy for the pods of the core eval jobs.
      restartPolicy: Never
      # -- Time-to-live after completion for the core eval jobs.
      ttlSecondsAfterFinished: 172800

  # -- Optional override for evaluation images.
  evaluationImages:
    # -- The image for the bigcode evaluation harness evaluation: nvcr.io/nvidia/eval-factory/bigcode-evaluation-harness
    BIGCODE_EVALUATION_HARNESS: ""
    # -- The image for the language model evaluation harness evaluation: nvcr.io/nvidia/eval-factory/lm-evaluation-harness
    LM_EVAL_HARNESS: ""
    # -- The image for the retriever evaluation: nvcr.io/nvidia/eval-factory/rag_retriever_eval
    RETRIEVER: ""
    # -- The image for the RAG evaluation: nvcr.io/nvidia/eval-factory/rag_retriever_eval
    RAG: ""
    # -- The image for the BFCL evaluation: nvcr.io/nvidia/eval-factory/bfcl
    BFCL: ""
    # -- The image for the Agentic evaluation: nvcr.io/nvidia/eval-factory/agentic_eval
    AGENTIC_EVAL: ""
    # -- The image for the model safety evaluation: nvcr.io/nvidia/eval-factory/safety-harness
    SAFETY_HARNESS: ""
    # -- The image for the Simple-Evals evaluation: nvcr.io/nvidia/eval-factory/simple-evals
    SIMPLE_EVALS: ""

  ingress:
    # -- Whether to enable the ingress for the NeMo Evaluator service.
    enabled: false
    # -- The ingress class name.
    className: ""
    # -- Annotations for the ingress resource.
    annotations: {}
    # -- (list) A list of maps, each containing the keys `host` and `paths` for the ingress resource. Specify hosts and paths as a list to configure ingress for the NeMo Evaluator service.
    # @default -- []
    hosts: {}
    #   - host: chart-example.local
    #     paths:
    #       - path: /
    #         pathType: ImplementationSpecific
    # -- TLS configuration for the ingress resource.
    tls: []

  # -- Resources for the NeMo Evaluator service.
  resources: {}

  livenessProbe:
    # -- Configures the liveness probe for the NeMo Evaluator microservice. The liveness probe checks if the container is running. The probe sends an HTTP GET request to the `/health` endpoint on the container's `http` port.
    httpGet:
      path: /health
      port: http

  readinessProbe:
    # -- Configures the readiness probe for the NeMo Evaluator microservice. The readiness probe checks if the container is ready to receive traffic. The probe sends an HTTP GET request to the `/health` endpoint on the container's `http` port.
    httpGet:
      path: /health
      port: http

  autoscaling:
    # -- Whether to enable autoscaling for the NeMo Evaluator microservice.
    enabled: false
    # -- The minimum number of replicas for the NeMo Evaluator microservice.
    minReplicas: 1
    # -- The maximum number of replicas for the NeMo Evaluator microservice.
    maxReplicas: 100
    # -- The target CPU utilization percentage for the NeMo Evaluator microservice.
    targetCPUUtilizationPercentage: 80

  # -- Volumes for the NeMo Evaluator microservice.
  volumes: []
  # -- Volume mounts for the NeMo Evaluator microservice.
  volumeMounts: []

  # -- Node selector for the NeMo Evaluator microservice.
  nodeSelector: {}

  # -- Tolerations for the NeMo Evaluator microservice.
  tolerations: []

  # -- Affinity for the NeMo Evaluator microservice.
  affinity: {}

  # -- Host for the NeMo Evaluator microservice.
  evaluator:
    host: "0.0.0.0"

  # -- Whether to enable the OpenTelemetry exporter for the NeMo Evaluator microservice.
  otelExporterEnabled: false

  # -- Log level for the NeMo Evaluator microservice.
  logLevel: INFO

  # -- OpenTelemetry environment configuration variables for the NeMo Evaluator microservice.
  otelEnvVars:
    # OTEL_EXPORTER_OTLP_ENDPOINT: "http://$(HOST_IP):4317" # sends to gRPC receiver on port 4317
    OTEL_SERVICE_NAME: "nemo-evaluator"
    OTEL_TRACES_EXPORTER: otlp
    OTEL_METRICS_EXPORTER: otlp
    OTEL_LOGS_EXPORTER: otlp
    OTEL_PROPAGATORS: "tracecontext,baggage"
    OTEL_RESOURCE_ATTRIBUTES: "deployment.environment=$(NAMESPACE)"
    OTEL_PYTHON_EXCLUDED_URLS: "health"
    OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED: "true"

  external:
    dataStore:
      # -- The external URL of the NeMo Data Store microservice.
      endpoint: "http://nemo-data-store:3000/v1/hf"
    entityStore:
      # -- The external URL of the NeMo Entity Store microservice.
      endpoint: "http://nemo-entity-store:8000"
    nimProxy:
      # -- The external URL of the NeMo Nim Proxy microservice.
      endpoint: "http://nemo-nim-proxy:8000"
    milvus:
      # -- The external URL of your own Milvus service.
      endpoint: ""

  dbMigration:
    resources:
      limits:
        # -- The CPU resource limit for the DB Migration service.
        cpu: 1
        # -- The memory resource limit for the DB Migration service.
        memory: 1Gi

  postgresql:
    # -- Whether to enable the default PostgreSQL service. For more information about setting up a PostgreSQL service, see the [PostgreSQL setup guide](https://docs.nvidia.com/nemo/microservices/latest/set-up/manage-storage/database/postgres.html).
    enabled: true
    # -- The architecture for the default PostgreSQL service.
    architecture: standalone
    global:
      # -- The storage class for the default PostgreSQL service.
      storageClass: ""
      # -- The storage size for the default PostgreSQL service.
      size: 10Gi
    auth:
      # -- Whether to enable the PostgreSQL user.
      enablePostgresUser: true
      # -- The username for the PostgreSQL service.
      username: nemo
      # -- The password for the PostgreSQL service.
      password: nemo
      # -- The database for the PostgreSQL service.
      database: evaluation
      # -- The existing secret you want to use for the PostgreSQL service.
      existingSecret: ""
    primary:
      networkPolicy:
        # -- Specifies whether to enable the network policy for the PostgreSQL service.
        enabled: false
      service:
        ports:
          # -- The primary service port for the PostgreSQL service.
          postgresql: 5432
    # -- The name override for the default PostgreSQL database.
    nameOverride: evaluatordb
    serviceAccount:
      # -- The name of the service account for PostgreSQL.
      name: evaluator-postgresql

  postgresWaitImage:
    # -- The repository location of the image used to wait for postgres to start.
    repository: "busybox"
    # -- The tag of the image used when waiting.
    tag: "latest"

  externalDatabase:
    # -- The host for an external database.
    host: localhost
    # -- The port for the external database.
    port: 5432
    # -- The user for the external database.
    user: nemo
    # -- The database for the external database.
    database: evaluation
    # -- The existing secret for the external database.
    existingSecret: ""
    # -- The existing secret password key for the external database.
    existingSecretPasswordKey: ""
    # -- The name for the external database secret.
    uriSecret:
      name: ""
      key: ""

  zipkin:
    # -- Whether to enable the default Zipkin service.
    enabled: false
    # -- The full name override for the default Zipkin service.
    fullnameOverride: "evaluator-zipkin"

  opentelemetry-collector:
    # -- Whether to enable the OpenTelemetry Collector service.
    enabled: false
    # -- The mode for the OpenTelemetry Collector service.
    mode: deployment
    config:
      receivers:
        # -- The OTLP receiver for the OpenTelemetry Collector service.
        otlp:
          protocols:
            grpc:
            http:
              cors:
                allowed_origins:
                  - "*"
      exporters:
        # NOTE: Prior to v0.86.0 use `logging` instead of `debug`.
        # -- The Zipkin exporter for the OpenTelemetry Collector service.
        zipkin:
          endpoint: "http://zipkin:9411/api/v2/spans"
        # -- Debugging verbosity for the OpenTelemetry Collector service.
        debug:
          verbosity: detailed
      processors:
        # -- The batch processor for the OpenTelemetry Collector service.
        batch:  {}
        # -- The tail sampling processor for the OpenTelemetry Collector service.
        tail_sampling:
          # filter out health checks
          # https://github.com/open-telemetry/opentelemetry-collector/issues/2310#issuecomment-1268157484
          # -- The policies for the OpenTelemetry Collector service.
          policies:
            - name: drop_noisy_traces_url
              type: string_attribute
              string_attribute:
                key: http.target
                values:
                  - \/health
                enabled_regex_matching: true
                invert_match: true
        # -- The transform processor configuration for the OpenTelemetry Collector service.
        transform:
          # -- The trace statements for the OpenTelemetry Collector service.
          trace_statements:
            - context: span
              statements:
                - set(status.code, 1) where attributes["http.path"] == "/health"
                # here, you can add code to replace span names or attributes for your own needs.
      service:
        pipelines:
          # -- The traces pipeline for the OpenTelemetry Collector service.
          traces:
            # -- The receivers for the traces pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the traces pipeline for the OpenTelemetry Collector service.
            exporters: [debug, zipkin]
            # -- The processors for the traces pipeline for the OpenTelemetry Collector service.
            processors: [tail_sampling, transform]
          # -- The metrics pipeline for the OpenTelemetry Collector service.
          metrics:
            # -- The receivers for the metrics pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the metrics pipeline for the OpenTelemetry Collector service.
            exporters: [debug]
            # -- The processors for the metrics pipeline for the OpenTelemetry Collector service.
            processors: [batch]
          # -- The logs pipeline for the OpenTelemetry Collector service.
          logs:
            # -- The receivers for the logs pipeline for the OpenTelemetry Collector service.
            receivers: [otlp]
            # -- The exporters for the logs pipeline for the OpenTelemetry Collector service.
            exporters: [debug]
            # -- The processors for the logs pipeline for the OpenTelemetry Collector service.
            processors: [batch]

  milvus:
    # -- Whether to enable the default Milvus service. Enable this for RAG and Retriever pipelines. For more information about setting up a Milvus service, see the [Milvus setup guide](https://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/evaluator.html#configure-milvus).
    enabled: false
    # -- The service name for the default Milvus service.
    serviceName: milvus
    service:
      # -- The service port for the default Milvus service.
      port: 19121
    cluster:
      # -- Whether to enable the default Milvus cluster.
      enabled: false
    etcd:
      # -- Whether to enable the etcd for the default Milvus service.
      enabled: false
    pulsar:
      # -- Whether to enable the Pulsar for the default Milvus service.
      enabled: false
    minio:
      # -- Whether to enable the Minio for the default Milvus service.
      enabled: false
      # -- Whether to enable the TLS for the Minio service.
      tls:
        enabled: false
    standalone:
      # -- Whether to enable the standalone for the default Milvus service.
      persistence:
        # -- Whether to enable the persistence for the default Milvus service.
        enabled: true
        # -- The persistent volume claim for the default Milvus service.
        persistentVolumeClaim:
          # -- The size for the persistent volume claim for the default Milvus service.
          size: 50Gi
          # -- The storage class for the persistent volume claim for the default Milvus service.
          storageClass: ""
      # -- Extra environment variables for the default Milvus service.
      extraEnv:
        - name: LOG_LEVEL
          value: error
    # -- Extra configuration files for the default Milvus service.
    extraConfigFiles:
      user.yaml: |+
        etcd:
          use:
            embed: true
          data:
            dir: /var/lib/milvus/etcd
        common:
          storageType: local

  # -- Sets a predictable name for the NeMo Evaluator Kubernetes service
  serviceName: nemo-evaluator

guardrails:
  # -- Number of replicas for the NeMo Guardrails microservice deployment.
  replicaCount: 1

  image:
    # -- The repository location of the NeMo Guardrails container image.
    repository: nvcr.io/nvidia/nemo-microservices/guardrails
    # -- The tag of the NeMo Guardrails container image.
    tag: ""
    # -- The image pull policy for the NeMo Guardrails container image.
    pullPolicy: IfNotPresent

  # -- Specifies the list of secret names that are needed for the main container and any init containers.
  imagePullSecrets:
    - name: nvcrimagepullsecret

  # -- Overrides the chart name.
  nameOverride: ""

  # -- Overrides the full chart name.
  fullnameOverride: ""

  # -- Kubernetes secret containing NVIDIA_API_KEY for Guardrails to use Nemoguard NIMS on NVCF
  guardrails:
    nvcfAPIKeySecretName: "ngc-api"

  # -- Environment variables for the container.
  env:
    # -- The NIM endpoint URL for the NeMo Guardrails microservice.
    NIM_ENDPOINT_URL: http://nemo-nim-proxy:8000/v1
    CONFIG_STORE_PATH: "/app/services/guardrails/config-store"
    DEMO: "True"
    DEFAULT_CONFIG_ID: self-check
    DEFAULT_LLM_PROVIDER: "nim"
    NEMO_GUARDRAILS_SERVER_ENABLE_CORS: "False"
    NEMO_GUARDRAILS_SERVER_ALLOWED_ORIGINS: "*"
    FETCH_NIM_APP_MODELS: "True"
    GUARDRAILS_HOST: "0.0.0.0"
    GUARDRAILS_PORT: "7331"

    # -- External service endpoints configuration.
  external:
    entityStore:
      # -- The external URL of the NeMo Entity Store microservice.
      endpoint: "http://nemo-entity-store:8000"

  # -- Specifies the service type and the port for the deployment.
  service:
    type: ClusterIP
    port: 7331

  configStore:
    nfs:
      # -- Whether to enable the use of an NFS persistent volume for the configuration store.
      enabled: false
      # -- The path to the root of the Configuration Store folder.
      path: "/path/to/nfs/share"
      # -- The address of the NFS server.
      server: "nfs-server.example.com"
      # -- The path where the NFS volume will be mounted inside the container.
      mountPath: "/config-store"
      # -- The storage class for the PV and PVC.
      storageClass: "standard"

  readinessProbe:
    # -- The HTTP GET request to use for the readiness probe.
    httpGet:
      path: /v1/health
      port: 7331
    # -- The initial delay seconds for the readiness probe.
    initialDelaySeconds: 5
    # -- The timeout in seconds for the readiness probe.
    timeoutSeconds: 30

  livenessProbe:
    # -- The HTTP GET request to use for the liveness probe.
    httpGet:
      path: /v1/health
      port: 7331
    # -- The initial delay seconds for the liveness probe.
    initialDelaySeconds: 5
    # -- The timeout in seconds for the liveness probe.
    timeoutSeconds: 30

  serviceAccount:
    # -- Whether to create a service account for the NeMo Guardrails microservice.
    create: true
    # -- Whether to automatically mount the service account token.
    automount: true
    # -- Annotations to be added to the service account.
    annotations: {}
    # -- The name of the service account to use.
    name: ""

  # -- Specifies additional annotations to the main deployment pods.
  podAnnotations: {}

  # -- Specifies additional labels to the main deployment pods.
  podLabels: {}

  # -- Specifies privilege and access control settings for the pod.
  podSecurityContext: {}
  # -- Specifies the group ID for the pod.
  runAsGroup: ""
  # -- Specifies the file system owner group id.
  fsGroup: ""

  # -- Specifies security context for the container.
  securityContext: {}

  ingress:
    # -- Whether to enable the ingress resource.
    enabled: false
    # -- The class name for the ingress resource.
    className: ""
    # -- Additional annotations for the ingress resource.
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    # -- (list) A list of maps, each containing the keys `host` and `paths` for the ingress resource. You must specify a list for configuring ingress for the microservice.
    # @default -- []
    hosts: {}
    #   - host: chart-example.local
    #     paths:
    #       - path: /
    #         pathType: ImplementationSpecific
    #         serviceType: openai # can be nemo or openai -- make sure your model serves the appropriate port(s)
    # -- TLS configuration for the ingress resource.
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  # -- Specifies resource configurations for the deployment.
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  # -- Specifies autoscaling configurations for the deployment.
  autoscaling:
    # -- Whether to enable horizontal pod autoscaler.
    enabled: false
    # -- The minimum number of replicas for the deployment.
    minReplicas: 1
    # -- The maximum number of replicas for the deployment.
    maxReplicas: 100
    # -- The target CPU utilization percentage.
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

    # -- PostgreSQL configuration for the NeMo Guardrails microservice.
  postgresql:
    # -- Whether to install the default PostgreSQL Helm chart. If enabled, the NeMo Guardrails microservice Helm chart uses the [PostgreSQL Helm chart from Bitnami](https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml) to create a PostgreSQL database.
    enabled: true
    # -- The name override for the Guardrails PostgreSQL database.
    nameOverride: guardrailsdb
    # -- The architecture for the default PostgreSQL service.
    architecture: standalone
    serviceAccount:
      # -- The name of the service account for PostgreSQL.
      name: guardrails-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true
    auth:
      # -- Whether to assign a password to the "postgres" admin user. If disabled, remote access is blocked for this user.
      enablePostgresUser: true
      # -- The user name to use for the PostgreSQL database.
      username: guardrails
      # -- The password for the PostgreSQL user.
      password: guardrails
      # -- The name for a custom database to create.
      database: nemo-guardrails
      # -- The name of an existing secret to use for PostgreSQL credentials.
      existingSecret: ""

  postgresWaitImage:
    # -- The repository location of the image used to wait for postgres to start.
    repository: "busybox"
    # -- The tag of the image used when waiting.
    tag: "latest"

  # -- External PostgreSQL configuration.
  externalDatabase:
    # -- The database host.
    host: ""
    # -- The database port number.
    port: ""
    # -- The username for the NeMo Guardrails external database.
    user: ""
    # -- The name of the database for the NeMo Guardrails service.
    database: ""
    # -- The name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- The name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    uriSecret:
      # -- The name of an existing secret that includes a full database URI.
      name: ""
      # -- The key within the existing secret that includes a full database URI.
      key: ""

  # -- Specifies labels to ensure that the NeMo Guardrails microservice is deployed only on certain nodes. To learn more, refer to the [Node Selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) in the Kubernetes documentation.
  nodeSelector: {}

  # -- Specifies tolerations for pod assignment. To learn more, refer to the [Taint and Toleration](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) in the Kubernetes documentation.
  tolerations: []

  # -- Specifies affinity settings for the deployment. To learn more, refer to the [Affinity and Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) in the Kubernetes documentation.
  affinity: {}
  # -- The service name for the NeMo Guardrails microservice.
  serviceName: nemo-guardrails

nemo-operator:
  # -- Whether to watch all namespaces, default is to restrict the NeMo Operator microservice to watch resources in the NMP deployment namespace.
  watchAllNamespaces: true

  # -- Additional environment variables to pass to controller manager. The format is `NAME: value` or `NAME: valueFrom: {object}`.
  env:
    {}

  controllerManager:
    serviceAccount:
      # -- Annotations to add to the service account.
      annotations: {}
    kubeRbacProxy:
      # -- Arguments to pass to the `kube-rbac-proxy` container.
      args:
      - --secure-listen-address=0.0.0.0:8443
      - --upstream=http://127.0.0.1:8080/
      - --logtostderr=true
      - --v=0
      containerSecurityContext:
        # -- Whether to allow container privilege escalation.
        allowPrivilegeEscalation: false
        capabilities:
          # -- List of capabilities to drop.
          drop:
          - ALL
      image:
        # -- The repository for the `kube-rbac-proxy` image.
        repository: gcr.io/kubebuilder/kube-rbac-proxy
        # -- The tag for the `kube-rbac-proxy` image.
        tag: v0.15.0
      resources:
        limits:
          # -- The CPU limit for the `kube-rbac-proxy` container.
          cpu: 500m
          # -- The memory limit for the `kube-rbac-proxy` container.
          memory: 128Mi
        requests:
          # -- The CPU request for the `kube-rbac-proxy` container.
          cpu: 5m
          # -- The memory request for the `kube-rbac-proxy` container.
          memory: 64Mi
    manager:
      # -- The scheduler to use for the controller manager.
      scheduler: volcano
      # -- Arguments to pass to the manager container.
      args:
      - --health-probe-bind-address=:8081
      - --metrics-bind-address=127.0.0.1:8080
      - --leader-elect
      - --leader-election-id=nemo.nko.nvidia.com
      containerSecurityContext:
        # -- Whether to allow container privilege escalation.
        allowPrivilegeEscalation: false
        capabilities:
          # -- List of capabilities to drop.
          drop:
          - ALL
      image:
        # -- The repository for the NeMo Operator microservice image.
        repository: nvcr.io/nvidia/nemo-microservices/nemo-operator
        # -- The tag for the NeMo Operator microservice image. The default value is `appVersion` from the `Chart.yaml` file.
        tag: ""
      resources:
        limits:
          # -- The CPU limit for the operator manager container.
          cpu: 1024m
          # -- The memory limit for the operator manager container.
          memory: 2Gi
        requests:
          # -- The CPU request for the operator manager container.
          cpu: 512m
          # -- The memory request for the operator manager container.
          memory: 1Gi
    # -- The number of operator replicas to run.
    replicas: 1

  # -- (list) Image pull secrets for accessing the NGC container registry.
  imagePullSecrets:
    - name: nvcrimagepullsecret
  # -- The Kubernetes cluster domain.
  kubernetesClusterDomain: cluster.local

  metricsService:
    # -- Whether to enable the metrics service for the NeMo Operator microservice. If you enable it, the microservice exposes a metrics endpoint for Prometheus.
    # Before installing this chart, you should have Prometheus installed in your environment.
    enabled: true
    # -- The metrics service ports configuration.
    ports:
    - name: https
      port: 8443
      protocol: TCP
      targetPort: https
    # -- The type of the metrics service.
    type: ClusterIP

nim-operator:
  nfd:
    nodeFeatureRules:
      # -- Specifies whether to enable device ID feature rules.
      deviceID: false

entity-store:
  # -- The number of NeMo Entity Store replicas to deploy.
  replicaCount: 1

  image:
    # -- The NeMo Entity Store image repository.
    repository: nvcr.io/nvidia/nemo-microservices/entity-store
    # -- The image pull policy to pull the NeMo Entity Store image
    pullPolicy: IfNotPresent
    # -- Specifies the image tag.
    tag: ""

  # -- List of image pull secrets. You can add multiple secrets to the list.
  imagePullSecrets:
    - name: nvcrimagepullsecret
  # -- String to partially override name on resulting Kubernetes objects when deployed.
  nameOverride: ""
  # -- String to fully override name on resulting Kubernetes objects when deployed.
  fullnameOverride: ""

  # -- Additional environment variables to pass to the NeMo Entity Store container. Format should be `NAME: value` or `NAME: valueFrom: {object}`.
  env:
    {}

  serviceAccount:
    # -- Whether to automatically mount the service account's API credentials.
    automount: true
    # -- Additional custom annotations for the service account.
    annotations: {}
    # -- The name of the service account to use.
    name: ""

  # -- Additional annotations for the NeMo Entity Store pods.
  podAnnotations: {}
  # -- Additional labels for the NeMo Entity Store pods.
  podLabels: {}

  # -- The pod security context for the NeMo Entity Store pods.
  podSecurityContext: {}

  # -- The security context for the NeMo Entity Store pods.
  securityContext: {}

  service:
    # -- The Kubernetes service type of the NeMo Entity Store microservice.
    type: ClusterIP
    # -- The service port for the NeMo Entity Store microservice.
    port: 8000

  # -- The name of the Kubernetes Service created for NeMo Entity Store.
  serviceNameOverride: ""

  ingress:
    # -- Whether to enable ingress.
    enabled: false
    # -- The name of the ingress class.
    className: ""
    # -- Additional ingress annotations.
    annotations: {}
    # -- The default host name for the ingress record.
    hostname: ""
    # -- (list) A list of maps, each containing the keys `host` and `paths` for the ingress resource. You must specify a list for configuring ingress for the microservice.
    # @default -- []
    hosts: {}
    #   - host: ""
    #     paths:
    #       - path: /
    #         pathType: ImplementationSpecific
    # -- The TLS configuration for the ingress.
    tls: []

  openTelemetry:
    # -- Whether to enable OpenTelemetry integration.
    enabled: false

  livenessProbe:
    # -- The path for the liveness probe.
    httpGet:
      path: /health
      port: http
    # -- The initial delay seconds for the Kubernetes liveness probe.
    initialDelaySeconds: 3
    # -- The period seconds for the liveness probe.
    periodSeconds: 10
    # -- The timeout seconds for the liveness probe.
    timeoutSeconds: 20
    # -- The failure threshold for the liveness probe.
    failureThreshold: 10

  readinessProbe:
    # -- The path for the readiness probe.
    httpGet:
      path: /health
      port: http
    # -- The initial delay seconds for the readiness probe.
    initialDelaySeconds: 10
    # -- The period seconds for the readiness probe.
    periodSeconds: 10
    # -- The timeout seconds for the readiness probe.
    timeoutSeconds: 20
    # -- The failure threshold for the readiness probe.
    failureThreshold: 20

  # -- Requests and limits for underlying Kubernetes deployment for NeMo Entity Store.
  resources: {}
    # If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  autoscaling:
    # -- Whether to enable autoscaling.
    enabled: false
    # -- The minimum number of replicas.
    minReplicas: 1
    # -- The maximum number of replicas.
    maxReplicas: 100
    # -- The target CPU utilization percentage for autoscaling.
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # -- Additional volumes for the deployment.
  volumes: []

  # -- Additional volume mounts for the deployment.
  volumeMounts: []

  # -- Additional NeMo Entity Store API server specific configs.
  appConfig:
    # -- The base URL for the NIM service used by the NeMo Entity Store microservice.
    BASE_URL_NIM: http://nemo-nim-proxy:8000
    # -- The base URL for the NeMo Data Store microservice.
    BASE_URL_DATASTORE: http://nemo-data-store:3000/v1/hf

  # -- Additional node selector configuration for the deployment.
  nodeSelector: {}

  # -- Additional tolerations for the deployment.
  tolerations: []

  # -- Additional affinity for the deployment.
  affinity: {}

  postgresql:
    # -- Whether to install the default PostgreSQL Helm chart. If enabled, the NeMo Entity Store microservice Helm chart uses the [PostgreSQL Helm chart from Bitnami](https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml) to create a PostgreSQL database.
    enabled: true
    # -- The name override for the PostgreSQL database.
    nameOverride: entity-storedb
    serviceAccount:
      # -- The service account name for PostgreSQL.
      name: entity-store-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true
    auth:
      # -- Whether to assign a password to the "postgres" admin user. If disabled, remote access is blocked for this user.
      enablePostgresUser: true
      # -- The user name to use for the PostgreSQL database.
      username: user
      # -- The password for the PostgreSQL user.
      password: pass
      # -- The name for a custom database to create.
      database: entity-store
      # -- The name of an existing secret to use for PostgreSQL credentials.
      existingSecret: ""
    # -- The PostgreSQL architecture. Available options are `standalone` or `replication`.
    architecture: standalone

  postgresWaitImage:
    # -- The repository location of the image used to wait for postgres to start.
    repository: "busybox"
    # -- The tag of the image used when waiting.
    tag: "latest"

  externalDatabase:
    # -- The database host.
    host: localhost
    # -- The database port number.
    port: 5432
    # -- The database username for the NeMo Entity Store service.
    user: user
    # -- The name of the database for the NeMo Entity Store service.
    database: entity-store
    # -- The name of an existing secret resource containing the database credentials.
    existingSecret: ""
    # -- The name of an existing secret key containing the database credentials.
    existingSecretPasswordKey: ""
    uriSecret:
      # -- The name of an existing secret that includes a full database URI.
      name: ""
      # -- The key within the existing secret that includes a full database URI.
      key: ""

  demo:
    # -- Whether to enable demo mode.
    enabled: false
    # -- The name of the NGC image pull secret for demo mode.
    ngcImagePullSecret: nvcrimagepullsecret
    # -- The NGC API key for demo mode.
    ngcApiKey: ""
  # -- The service name for the NeMo Entity Store microservice.
  serviceName: nemo-entity-store

volcano:
  # -- Specifies whether to enable the default Volcano scheduler installation. To learn more, see [Volcano](https://docs.nvidia.com/nemo/microservices/latest/set-up/deploy-as-microservices/customizer.html#volcano).
  enabled: true

deployment-management:
  # -- The number of replicas for the NeMo Deployment Management service.
  replicaCount: 1

  # -- Specifies a namespace to restrict the NeMo Deployment Management microservice to watch NIMs in. Leave it empty to watch all namespaces.
  nimNamespace: ""

  image:
    # -- The repository of the NeMo Deployment Management container image.
    repository: nvcr.io/nvidia/nemo-microservices/deployment-management
    # -- The container image pull policy for the NeMo Deployment Management container.
    pullPolicy: IfNotPresent
    # -- The container image tag. If not set, the default value is `appVersion` from the `Chart.yaml` file.
    tag: ""

  # -- (list) Image pull secrets for accessing the NGC container registry.
  imagePullSecrets:
    - name: nvcrimagepullsecret
  # -- String to partially override name on resulting Kubernetes objects when the NeMo Deployment Management microservice is deployed.
  nameOverride: ""
  # -- String to fully override the name on resulting Kubernetes objects when the NeMo Deployment Management microservice is deployed.
  fullnameOverride: ""

  # -- Additional environment variables to pass to the NeMo Deployment Management microservice container. The format is `NAME: value` or `NAME: valueFrom: {object}`.
  env:
    {}
  # -- Configures service account for RBAC for the NeMo Deployment Management microservice. Use the default setup, unless you understand what changes in RBAC settings you want to apply to the service.
  serviceAccount:
    # -- Whether to create a service account for the NeMo Deployment Management microservice. This is for setting RBAC up.
    create: true
    # -- Automatically mount a ServiceAccount's API credentials.
    automount: true
    # -- Annotations to add to the service account.
    annotations: {}
    # -- The name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    name: ""

  # -- Pod annotations.
  podAnnotations: {}
  # -- Pod labels.
  podLabels: {}

  # -- Pod security context settings. Use the default settings, unless you understand what changes in the pod security context settings you want to apply.
  podSecurityContext: {}
    # fsGroup: 2000

  # -- Security context settings. Use the default settings, unless you understand what changes in the security context settings you want to apply.
  securityContext:
    # -- Enable read-only root filesystem. You can also add any values for [Kubernetes security context](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#securitycontext-v1-core) in this field.
    readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    deploymentPort:
      # -- The service type of the deployment port.
      type: ClusterIP
      # -- The port number for the service.
      port: 8000

  ingress:
    # -- Whether to enable the ingress controller resource.
    enabled: false
    # -- The ingress class to use.
    className: ""
    # -- Ingress annotations.
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    # -- (list) A list of maps, each containing the keys `host` and `paths` for the ingress resource. You must specify a list for configuring ingress for the microservice.
    # @default -- []
    hosts: {}
    #   - host: chart-example.local
    #     paths:
    #       - path: /
    #         pathType: ImplementationSpecific
    # -- Ingress TLS configuration.
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  # -- Configures Kubernetes resource requests and limits for the NeMo Deployment Management microservice. Use the default settings and leave it with the empty object as is, unless you understand what changes you want to make.
  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  livenessProbe:
    httpGet:
      # -- The HTTP path for the Kubernetes liveness probe. Do not change this.
      path: /health
      # -- The port name for the Kubernetes liveness probe. Do not change this.
      port: http
  readinessProbe:
    httpGet:
      # -- The HTTP path for the Kubernetes readiness probe. Do not change this.
      path: /health
      # -- The port name for the Kubernetes readiness probe. Do not change this.
      port: http

  autoscaling:
    # -- Whether to enable autoscaling for the NeMo Deployment Management microservice.
    enabled: false
    # -- The minimum number of replicas.
    minReplicas: 1
    # -- The maximum number of replicas.
    maxReplicas: 100
    # -- The target CPU utilization percentage.
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # -- Additional volumes for the NeMo Deployment Management microservice. Use the default settings and leave it with the empty list as is, unless you understand what changes you want to make.
  volumes: []

  # -- Additional volume mounts for the NeMo Deployment Management microservice. Use the default settings and leave it with the empty list as is, unless you understand what changes you want to make.
  volumeMounts: []

  # -- Specifies labels to ensure that the microservice is deployed only on certain nodes. To learn more, refer to the [Node Selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) in the Kubernetes documentation.
  nodeSelector: {}

  # -- Specifies tolerations for pod assignment. To learn more, refer to the [Taint and Toleration](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) in the Kubernetes documentation.
  tolerations: []

  # -- Specifies affinity settings for the deployment. To learn more, refer to the [Affinity and Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) in the Kubernetes documentation.
  affinity: {}

  # -- Prometheus metrics collection configuration for monitoring the NeMo Deployment Management microservice. To enable this, you should install the Prometheus operator before deploying the NeMo Deployment Management microservice.
  monitoring:
    # -- Whether to enable monitoring for the NeMo Deployment Management microservice.
    enabled: false

  dataStore:
    # -- The URL for the NeMo Data Store service.
    url: http://nemo-data-store:3000
    secret:
      # -- Whether to create a secret for the data store huggingface token. If false, the secret must be created manually.
      create: true
      # -- The name of the secret that will be created. If create is false, a secret with this name must be created manually and have the HF_TOKEN key set.
      name: "nemo-deployment-management-service-ds-hf-token"

  # -- Properties to configure NIM deployments for the NeMo Deployment Management microservice.
  deployments:
    # -- The default storage class for NIM deployments.
    # -- Storage classes utilizing NFSv3+ may encounter issues with huggingface-cli file locking.
    defaultStorageClass: ""
    # -- The URL for PEFT model sources (typically points to NeMo Entity Store)
    nimPeftSource: http://nemo-entity-store:8000
    # -- The URL for the NeMo Entity Store service.
    entityStoreUrl: http://nemo-entity-store:8000
    # -- The period in seconds for model synchronization.
    modelSyncPeriod: "30"
    metrics:
      # -- Whether to enable metrics collection for the NIM deployments.
      enabled: false
    # -- The size of the PVC for the NIM deployments.
    nimPvcSize: 200Gi
    # -- The image to use for pulling models from NeMo Data Store. Must have the huggingface-cli binary installed.
    modelPullerImage: nvcr.io/nvidia/nemo-microservices/nds-v2-huggingface-cli:25.06
    # -- The pull secret used to pull the model puller image.
    modelPullerPullSecret: nvcrimagepullsecret
    autoscaling:
      # -- Whether to enable autoscaling for the NIM deployments.
      enabled: false
      # -- Autoscaling specification for the NIM deployments.
      spec:
        maxReplicas: 5
        metrics:
        - pods:
            metric:
              name: gpu_cache_usage_perc
            target:
              averageValue: 750m
              type: AverageValue
          type: Pods
        minReplicas: 1
    # -- List of image pull secrets for the NIM deployments.
    nimImagePullSecrets:
      - nvcrimagepullsecret
  # -- The service name for the NeMo Deployment Management microservice.
  serviceName: nemo-deployment-management

nim-proxy:
  # -- The number of replicas for the NeMo NIM Proxy service.
  replicaCount: 1

  # -- Specifies a namespace to restrict the NIM Proxy microservice to watch NIMs in. Leave it empty to watch all namespaces.
  nimNamespace: ""

  # -- Additional environment variables to pass to containers. The format is `NAME: value` or `NAME: valueFrom: {object}`.
  env:
    {}

  image:
    # -- The repository of the NIM Proxy container image.
    repository: nvcr.io/nvidia/nemo-microservices/nim-proxy
    # -- The container image pull policy for the NIM Proxy container.
    pullPolicy: IfNotPresent
    # -- The container image tag. If not set, the default value is `appVersion` from the `Chart.yaml` file.
    tag: ""

  # -- (list) Image pull secrets for accessing the NGC container registry.
  imagePullSecrets: []
  # -- String to partially override name on resulting Kubernetes objects when the NIM Proxy microservice is deployed.
  nameOverride: ""
  # -- String to fully override the name on resulting Kubernetes objects when the NIM Proxy microservice is deployed.
  fullnameOverride: ""

  serviceAccount:
    # -- Whether to create a service account for the NIM Proxy microservice.
    create: true
    # -- Whether to automatically mount a ServiceAccount's API credentials.
    automount: true
    # -- Annotations to add to the service account.
    annotations: {}
    # -- The name of the service account to use. If not set and create is `true`, a name is generated using the full name template.
    name: ""
  # -- Pod annotations.
  podAnnotations: {}
  # -- Pod labels.
  podLabels: {}
  # -- Pod security context. Use the default settings, unless you understand what changes in the pod security context settings you want to apply.
  podSecurityContext: {}
    # fsGroup: 2000

  # -- Security context. Use the default settings, unless you understand what changes in the security context settings you want to apply.
  securityContext:
    # capabilities:
    #   drop:
    #   - ALL
    # -- Whether to run with a read-only root filesystem.
    readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    main:
      # -- The type of the main service.
      type: ClusterIP
      # -- The port of the main service.
      port: 8000
    metricsPort:
      # -- The type of the metrics service.
      type: ClusterIP
      # -- The port of the metrics service.
      port: 8001

  ingress:
    # -- Whether to enable the ingress controller resource.
    enabled: false
    # -- The ingress class to use.
    className: ""
    # -- Ingress annotations.
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    # -- (list) A list of maps, each containing the keys `host` and `paths` for the ingress resource. You must specify a list for configuring ingress for the microservice.
    # @default -- []
    hosts: {}
    #   - host: chart-example.local
    #     paths:
    #       - path: /
    #         pathType: ImplementationSpecific
    # -- Ingress TLS configuration.
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  # -- Configures Kubernetes resource requests and limits for the NIM Proxy microservice. Use the default settings and leave it with the empty object as is, unless you understand what changes you want to make.
  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  livenessProbe:
    httpGet:
      # -- The HTTP path for the Kubernetes liveness probe. Do not change this.
      path: /health
      # -- The port name for the Kubernetes liveness probe. Do not change this.
      port: http

  readinessProbe:
    httpGet:
      # -- The HTTP path for the Kubernetes readiness probe. Do not change this.
      path: /health
      # -- The port name for the Kubernetes readiness probe. Do not change this.
      port: http

  autoscaling:
    # -- Whether to enable horizontal pod autoscaling for the NIM Proxy microservice.
    enabled: false
    # -- The minimum number of replicas for the NIM Proxy microservice.
    minReplicas: 1
    # -- The maximum number of replicas for the NIM Proxy microservice.
    maxReplicas: 100
    # -- The target CPU utilization percentage for the NIM Proxy microservice.
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80

  # -- Additional volumes for the NIM Proxy microservice. Use the default settings and leave it with the empty list as is, unless you understand what changes you want to make.
  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  # -- Additional volume mounts for the NIM Proxy microservice. Use the default settings and leave it with the empty list as is, unless you understand what changes you want to make.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  # -- Specifies labels to ensure that the microservice is deployed only on certain nodes. To learn more, refer to [Node Selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) in the Kubernetes documentation.
  nodeSelector: {}

  # -- Specifies tolerations for pod assignment. To learn more, refer to [Taint and Toleration](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) in the Kubernetes documentation.
  tolerations: []

  # -- Specifies affinity settings for the deployment. To learn more, refer to [Affinity and Anti-Affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) in the Kubernetes documentation.
  affinity: {}

  # -- Prometheus metrics collection configuration for monitoring the NIM Proxy microservice. To enable this, you should install the Prometheus operator before deploying the NIM Proxy microservice.
  monitoring:
    # -- Whether to enable monitoring for the NIM Proxy microservice.
    enabled: false
    # -- The scrape interval for monitoring.
    interval: 30s
    # -- The metrics path for monitoring.
    path: /v1/metrics
    # -- The scheme for monitoring.
    scheme: http

  # -- The service name for the NIM Proxy microservice.
  serviceName: nemo-nim-proxy

ingress:
  # -- Specifies whether to enable the ingress.
  enabled: false
  # -- Annotations for the ingress resource.
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 50g
    ingress.kubernetes.io/proxy-body-size: 50g
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    cert-manager.io/subject-organizations: "Kubetee AI LTD"

  # -- The ingress class to use if your cluster has more than one class.
  className: ""
  # -- TLS configurations.
  tls: 
    - secretName: nemo-tls
      hosts:
        - nemo.kubetee.ai
        - nim.kubetee.ai
        - data-store.kubetee.ai
  # -- A map of hosts and their corresponding paths for the ingress.
  hosts:
    default:
      # -- The host name for the default ingress host.
      name: "nemo.kubetee.ai"
      # -- The path rules for the default ingress host.
      paths:
        - path: /v1/namespaces
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/projects
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/datasets
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/repos
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/models
          pathType: Prefix
          service: nemo-entity-store
          port: 8000
        - path: /v1/customization
          pathType: Prefix
          service: nemo-customizer
          port: 8000
        - path: /v1/evaluation
          pathType: Prefix
          service: nemo-evaluator
          port: 7331
        - path: /v1/guardrail
          pathType: Prefix
          service: nemo-guardrails
          port: 7331
        - path: /v1/deployment
          pathType: Prefix
          service: nemo-deployment-management
          port: 8000
        - path: /v1beta1/audit
          pathType: Prefix
          service: nemo-auditor
          port: 5000
    nimProxy:
      # -- The host name for the second ingress host for the NIM Proxy microservice.
      name: nim.kubetee.ai
      # -- The path rules for the second ingress host.
      paths:
        - path: /v1/completions
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
        - path: /v1/chat
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
        - path: /v1/embeddings
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
        - path: /v1/models
          pathType: Prefix
          service: nemo-nim-proxy
          port: 8000
    dataStore:
      # -- The host name for the third ingress host for the NeMo Data Store microservice.
      name: data-store.kubetee.ai
      # -- The path rules for the third ingress host.
      paths:
        - path: /
          pathType: Prefix
          service: nemo-data-store
          port: 3000

# -- Specifies whether to enable the virtual service. If you are not using istio and virtualservices, it can be useful to create some virtual services for the NeMo Microservices system. Don't enable unless you use istio.
# @default -- A virtual service configuration template.
virtualService:
  # -- Specifies whether to enable the virtual service.
  enabled: false
  # -- Labels for the virtual service.
  labels: {}
  # -- Annotations for the virtual service.
  annotations: {}
  main:
    # -- A list of gateways for the virtual service.
    gateways: []
    # -- A list of hosts for the virtual service.
    hosts: []
    entries:
      customizer:
        # -- The CORS policy for the virtual NeMo Customizer service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Customizer service.
        match:
          - uri:
              prefix: "/v1/customization"
        # -- The route for the virtual NeMo Customizer service.
        route:
          - destination:
              host: nemo-customizer
              port:
                number: 8000
      deployment-management:
        # -- The CORS policy for the virtual NeMo Deployment Management service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Deployment Management service.
        match:
          - uri:
              prefix: "/v1/deployment"
        # -- The route for the virtual NeMo Deployment Management service.
        route:
          - destination:
              host: nemo-deployment-management
              port:
                number: 8000
      entity-store:
        # -- The CORS policy for the virtual NeMo Entity Store service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Entity Store service.
        match:
          - uri:
              prefix: "/v1/namespaces"
          - uri:
              prefix: "/v1/projects"
          - uri:
              prefix: "/v1/datasets"
          - uri:
              prefix: "/v1/repos"
          - uri:
              prefix: "/v1/models"
        # -- The route for the virtual NeMo Entity Store service.
        route:
          - destination:
              host: nemo-entity-store
              port:
                number: 8000
      evaluator:
        # -- The CORS policy for the virtual NeMo Evaluator service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Evaluator service.
        match:
          - uri:
              prefix: "/v1/evaluation"
        # -- The route for the virtual NeMo Evaluator service.
        route:
          - destination:
              host: nemo-evaluator
              port:
                number: 7331
      guardrails:
        # -- The CORS policy for the virtual NeMo Guardrails service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Guardrails service.
        match:
          - uri:
              prefix: "/v1/guardrail"
        # -- The route for the virtual NeMo Guardrails service.
        route:
          - destination:
              host: nemo-guardrails
              port:
                number: 7331
      auditor:
        # -- The CORS policy for the virtual NeMo Auditor service.
        corsPolicy: {}
        # -- The match for the virtual NeMo Auditor service.
        match:
          - uri:
              prefix: /v1beta1/audit
        # -- The route for the virtual NeMo Auditor service.
        route:
          - destination:
              host: nemo-auditor
              port:
                number: 5000
  additional:
    # -- Additional virtual service configurations.
    data-store:
      # -- The gateways for the virtual NeMo Data Store service.
      gateways: []
      # -- The hosts for the virtual NeMo Data Store service.
      hosts: []
      # -- The entries for the virtual NeMo Data Store service.
      entries:
        data-store:
          # -- The CORS policy for the virtual NeMo Data Store service.
          corsPolicy: {}
          # -- The match for the virtual NeMo Data Store service.
          match:
            - uri:
                prefix: /
          # -- The route for the virtual NeMo Data Store service.
          route:
            - destination:
                host: nemo-data-store
                port:
                  number: 3000
    nim-proxy:
      # -- The gateways for the virtual NIM Proxy service.
      gateways: []
      # -- The hosts for the virtual NIM Proxy service.
      hosts: []
      entries:
        nim-proxy:
          # -- The CORS policy for the virtual NIM Proxy service.
          corsPolicy: {}
          # -- The match for the virtual NIM Proxy service.
          match:
            - uri:
                prefix: /
          # -- The route for the virtual NIM Proxy service.
          route:
            - destination:
                host: nemo-nim-proxy
                port:
                  number: 8000
auditor:
  # -- Specifies whether to install the NeMo Auditor microservice.
  serviceName: nemo-auditor

  # -- Number of replicas for the Auditor deployment.
  replicaCount: 1

  # -- Container image configuration.
  image:
    repository: nvcr.io/nvidia/nemo-microservices/auditor
    pullPolicy: IfNotPresent
    tag: ""

  # -- Service account configuration.
  serviceAccount:
    # -- Specifies whether a service account should be created.
    create: true
    # -- Automatically mount a ServiceAccount's API credentials.
    automount: true
    # -- Annotations to add to the service account.
    annotations: {}
    # -- The name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    name: ""

  # -- Annotations to add to pods.
  podAnnotations: {}
  # -- Labels to add to pods.
  podLabels: {}

  # -- Security context for the pod.
  podSecurityContext:
    fsGroup: 1000

  # -- Security context for the container.
  securityContext: {}

  # -- Service configuration.
  service:
    type: ClusterIP
    port: 5000

  # -- Istio virtual service configuration.
  virtualService:
    enabled: false
    dnsName: auditor.example.local
    gateway: istio-system/ingress-gateway
    apiPath: /v1beta1/audit
    annotations: {}

  # -- Ingress configuration.
  ingress:
    enabled: false
    className: "nginx"
    annotations: {}
    hosts: {}
    tls: []

  # -- Resource requests and limits.
  resources: {}

  # -- Liveness probe configuration.
  livenessProbe:
    httpGet:
      path: /health/live
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 600
    failureThreshold: 100

  # -- Readiness probe configuration.
  readinessProbe:
    httpGet:
      path: /health/ready
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 600
    failureThreshold: 100

  # -- Autoscaling configuration.
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 1
    targetCPUUtilizationPercentage: 80

  # -- Environment variables to pass to containers. This is an object formatted like NAME: value or NAME: valueFrom: {object}
  env: {}

  # -- Node selector for pod assignment.
  nodeSelector: {}

  # -- Affinity rules for pod assignment.
  affinity: {}

  # -- Tolerations for pod assignment.
  tolerations: []

  # -- Additional volumes on the output Deployment definition.
  volumes: []

  # -- Additional volume mounts on the output Deployment definition.
  volumeMounts: []

  # -- Name of secret with authenitcation key for build.nvidia.com, if used. This must have a key of NIM_API_KEY and with the actual key as the value.
  auditorApiKeysSecretName: ""

  # -- PostgreSQL configuration for the NeMo Auditor microservice.
  postgresql:
    # -- Whether to install the default PostgreSQL Helm chart. If enabled, the NeMo Auditor microservice Helm chart uses the [PostgreSQL Helm chart from Bitnami](https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml) to create a PostgreSQL database.
    enabled: true
    # -- The name override for the Auditor PostgreSQL database.
    nameOverride: auditdb
    # -- The architecture for the default PostgreSQL service.
    architecture: standalone
    serviceAccount:
      # -- The name of the service account for PostgreSQL.
      name: auditor-postgresql
      # -- Specifies whether to create a new service account for PostgreSQL.
      create: true
    auth:
      # -- Whether to assign a password to the "postgres" admin user. If disabled, remote access is blocked for this user.
      enablePostgresUser: true
      # -- The user name to use for the PostgreSQL database.
      username: nemo
      # -- The password for the PostgreSQL user.
      password: nemo
      # -- The name for a custom database to create.
      database: auditor
      # -- The name of an existing secret to use for PostgreSQL credentials.
      existingSecret: ""
    service:
      ports:
        postgresql: 5432
    persistence:
      enabled: true
      size: 10Gi

  # -- External PostgreSQL configuration.
  externalDatabase:
    # -- The database host.
    host: ""
    # -- The database port.
    port: 5432
    # -- The database user.
    user: nemo
    # -- The database name.
    database: auditor
    # -- The name of an existing secret to use for PostgreSQL credentials.
    existingSecret: ""
    # -- The name of an existing secret key to use for PostgreSQL credentials.
    existingSecretPasswordKey: ""
